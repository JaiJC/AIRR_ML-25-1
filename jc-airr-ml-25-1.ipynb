{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need for a uniform interface of running models\n",
    "\n",
    "As described in the official competition page, to win the prize money, a prerequisite is that the code has to be made open-source. In addition, the top 10 submissions/teams will be invited to become co-authors in a scientific paper that involves further stress-testing of their models in a subsequent phase with many other datasets outside Kaggle platform. **To enable such further analyses and re-use of the models by the community, we strongly encourage** the participants to adhere to a code template that we provide through this repository that enables a uniform interface of running models: [https://github.com/uio-bmi/predict-airr](https://github.com/uio-bmi/predict-airr)\n",
    "\n",
    "\n",
    "Ideally, all the methods can be run in a unified way, e.g.,\n",
    "\n",
    "`python3 -m submission.main --train_dir /path/to/train_dir --test_dirs /path/to/test_dir_1 /path/to/test_dir_2 --out_dir /path/to/output_dir --n_jobs 4 --device cpu`\n",
    "\n",
    "## Adhering to code template on Kaggle Notebooks\n",
    "\n",
    "Those participants who make use of Kaggle resources and Kaggle notebooks to develop and run their code are also strongly encouraged to copy the code template, particularly the `ImmuneStatePredictor` class and any utility functions from the provided code template repository and adhere to the code template to enable a unified way of running different methods at a later stage. In this notebook, we copied the code template below for participants to paste into their respective Kaggle notebooks and edit as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## imports required for the basic code template below.\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import sys\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "from typing import Iterator, Tuple, Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## some utility functions such as data loaders, etc.\n",
    "\n",
    "def load_data_generator(data_dir: str, metadata_filename='metadata.csv') -> Iterator[\n",
    "    Union[Tuple[str, pd.DataFrame, bool], Tuple[str, pd.DataFrame]]]:\n",
    "    \"\"\"\n",
    "    A generator to load immune repertoire data.\n",
    "\n",
    "    This function operates in two modes:\n",
    "    1.  If metadata is found, it yields data based on the metadata file.\n",
    "    2.  If metadata is NOT found, it uses glob to find and yield all '.tsv'\n",
    "        files in the directory.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): The path to the directory containing the data.\n",
    "\n",
    "    Yields:\n",
    "        An iterator of tuples. The format depends on the mode:\n",
    "        - With metadata: (repertoire_id, pd.DataFrame, label_positive)\n",
    "        - Without metadata: (filename, pd.DataFrame)\n",
    "    \"\"\"\n",
    "    metadata_path = os.path.join(data_dir, metadata_filename)\n",
    "\n",
    "    if os.path.exists(metadata_path):\n",
    "        metadata_df = pd.read_csv(metadata_path)\n",
    "        for row in metadata_df.itertuples(index=False):\n",
    "            file_path = os.path.join(data_dir, row.filename)\n",
    "            try:\n",
    "                repertoire_df = pd.read_csv(file_path, sep='\\t')\n",
    "                yield row.repertoire_id, repertoire_df, row.label_positive\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: File '{row.filename}' listed in metadata not found. Skipping.\")\n",
    "                continue\n",
    "    else:\n",
    "        search_pattern = os.path.join(data_dir, '*.tsv')\n",
    "        tsv_files = glob.glob(search_pattern)\n",
    "        for file_path in sorted(tsv_files):\n",
    "            try:\n",
    "                filename = os.path.basename(file_path)\n",
    "                repertoire_df = pd.read_csv(file_path, sep='\\t')\n",
    "                yield filename, repertoire_df\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read file '{file_path}'. Error: {e}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "\n",
    "def load_full_dataset(data_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads all TSV files from a directory and concatenates them into a single DataFrame.\n",
    "\n",
    "    This function handles two scenarios:\n",
    "    1. If metadata.csv exists, it loads data based on the metadata and adds\n",
    "       'repertoire_id' and 'label_positive' columns.\n",
    "    2. If metadata.csv does not exist, it loads all .tsv files and adds\n",
    "       a 'filename' column as an identifier.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): The path to the data directory.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A single, concatenated DataFrame containing all the data.\n",
    "    \"\"\"\n",
    "    metadata_path = os.path.join(data_dir, 'metadata.csv')\n",
    "    df_list = []\n",
    "    data_loader = load_data_generator(data_dir=data_dir)\n",
    "\n",
    "    if os.path.exists(metadata_path):\n",
    "        metadata_df = pd.read_csv(metadata_path)\n",
    "        total_files = len(metadata_df)\n",
    "        for rep_id, data_df, label in tqdm(data_loader, total=total_files, desc=\"Loading files\"):\n",
    "            data_df['ID'] = rep_id\n",
    "            data_df['label_positive'] = label\n",
    "            df_list.append(data_df)\n",
    "    else:\n",
    "        search_pattern = os.path.join(data_dir, '*.tsv')\n",
    "        total_files = len(glob.glob(search_pattern))\n",
    "        for filename, data_df in tqdm(data_loader, total=total_files, desc=\"Loading files\"):\n",
    "            data_df['ID'] = os.path.basename(filename).replace(\".tsv\", \"\")\n",
    "            df_list.append(data_df)\n",
    "\n",
    "    if not df_list:\n",
    "        print(\"Warning: No data files were loaded.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    full_dataset_df = pd.concat(df_list, ignore_index=True)\n",
    "    return full_dataset_df\n",
    "\n",
    "\n",
    "def load_and_encode_kmers(data_dir: str, k: int = 3) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Loading and k-mer encoding of repertoire data.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Path to data directory\n",
    "        k: K-mer length\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (encoded_features_df, metadata_df)\n",
    "        metadata_df always contains 'ID', and 'label_positive' if available\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    metadata_path = os.path.join(data_dir, 'metadata.csv')\n",
    "    data_loader = load_data_generator(data_dir=data_dir)\n",
    "\n",
    "    repertoire_features = []\n",
    "    metadata_records = []\n",
    "\n",
    "    search_pattern = os.path.join(data_dir, '*.tsv')\n",
    "    total_files = len(glob.glob(search_pattern))\n",
    "\n",
    "    for item in tqdm(data_loader, total=total_files, desc=f\"Encoding {k}-mers\"):\n",
    "        if os.path.exists(metadata_path):\n",
    "            rep_id, data_df, label = item\n",
    "        else:\n",
    "            filename, data_df = item\n",
    "            rep_id = os.path.basename(filename).replace(\".tsv\", \"\")\n",
    "            label = None\n",
    "\n",
    "        kmer_counts = Counter()\n",
    "        for seq in data_df['junction_aa'].dropna():\n",
    "            for i in range(len(seq) - k + 1):\n",
    "                kmer_counts[seq[i:i + k]] += 1\n",
    "\n",
    "        repertoire_features.append({\n",
    "            'ID': rep_id,\n",
    "            **kmer_counts\n",
    "        })\n",
    "\n",
    "        metadata_record = {'ID': rep_id}\n",
    "        if label is not None:\n",
    "            metadata_record['label_positive'] = label\n",
    "        metadata_records.append(metadata_record)\n",
    "\n",
    "        del data_df, kmer_counts\n",
    "\n",
    "    features_df = pd.DataFrame(repertoire_features).fillna(0).set_index('ID')\n",
    "    features_df.fillna(0)\n",
    "    metadata_df = pd.DataFrame(metadata_records)\n",
    "\n",
    "    return features_df, metadata_df\n",
    "\n",
    "\n",
    "def save_tsv(df: pd.DataFrame, path: str):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    df.to_csv(path, sep='\\t', index=False)\n",
    "\n",
    "\n",
    "def get_repertoire_ids(data_dir: str) -> list:\n",
    "    \"\"\"\n",
    "    Retrieves repertoire IDs from the metadata file or filenames in the directory.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): The path to the data directory.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of repertoire IDs.\n",
    "    \"\"\"\n",
    "    metadata_path = os.path.join(data_dir, 'metadata.csv')\n",
    "\n",
    "    if os.path.exists(metadata_path):\n",
    "        metadata_df = pd.read_csv(metadata_path)\n",
    "        repertoire_ids = metadata_df['repertoire_id'].tolist()\n",
    "    else:\n",
    "        search_pattern = os.path.join(data_dir, '*.tsv')\n",
    "        tsv_files = glob.glob(search_pattern)\n",
    "        repertoire_ids = [os.path.basename(f).replace('.tsv', '') for f in sorted(tsv_files)]\n",
    "\n",
    "    return repertoire_ids\n",
    "\n",
    "\n",
    "def generate_random_top_sequences_df(n_seq: int = 50000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a random DataFrame simulating top important sequences.\n",
    "\n",
    "    Args:\n",
    "        n_seq (int): Number of sequences to generate.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with columns 'ID', 'dataset', 'junction_aa', 'v_call', 'j_call'.\n",
    "    \"\"\"\n",
    "    seqs = set()\n",
    "    while len(seqs) < n_seq:\n",
    "        seq = ''.join(np.random.choice(list('ACDEFGHIKLMNPQRSTVWY'), size=15))\n",
    "        seqs.add(seq)\n",
    "    data = {\n",
    "        'junction_aa': list(seqs),\n",
    "        'v_call': ['TRBV20-1'] * n_seq,\n",
    "        'j_call': ['TRBJ2-7'] * n_seq,\n",
    "        'importance_score': np.random.rand(n_seq)\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def validate_dirs_and_files(train_dir: str, test_dirs: List[str], out_dir: str) -> None:\n",
    "    assert os.path.isdir(train_dir), f\"Train directory `{train_dir}` does not exist.\"\n",
    "    train_tsvs = glob.glob(os.path.join(train_dir, \"*.tsv\"))\n",
    "    assert train_tsvs, f\"No .tsv files found in train directory `{train_dir}`.\"\n",
    "    metadata_path = os.path.join(train_dir, \"metadata.csv\")\n",
    "    assert os.path.isfile(metadata_path), f\"`metadata.csv` not found in train directory `{train_dir}`.\"\n",
    "\n",
    "    for test_dir in test_dirs:\n",
    "        assert os.path.isdir(test_dir), f\"Test directory `{test_dir}` does not exist.\"\n",
    "        test_tsvs = glob.glob(os.path.join(test_dir, \"*.tsv\"))\n",
    "        assert test_tsvs, f\"No .tsv files found in test directory `{test_dir}`.\"\n",
    "\n",
    "    try:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        test_file = os.path.join(out_dir, \"test_write_permission.tmp\")\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create or write to output directory `{out_dir}`: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def concatenate_output_files(out_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Concatenates all test predictions and important sequences TSV files from the output directory.\n",
    "\n",
    "    This function finds all files matching the patterns:\n",
    "    - *_test_predictions.tsv\n",
    "    - *_important_sequences.tsv\n",
    "\n",
    "    and concatenates them to match the expected output format of submissions.csv.\n",
    "\n",
    "    Args:\n",
    "        out_dir (str): Path to the output directory containing the TSV files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated DataFrame with predictions followed by important sequences.\n",
    "                     Columns: ['ID', 'dataset', 'label_positive_probability', 'junction_aa', 'v_call', 'j_call']\n",
    "    \"\"\"\n",
    "    predictions_pattern = os.path.join(out_dir, '*_test_predictions.tsv')\n",
    "    sequences_pattern = os.path.join(out_dir, '*_important_sequences.tsv')\n",
    "\n",
    "    predictions_files = sorted(glob.glob(predictions_pattern))\n",
    "    sequences_files = sorted(glob.glob(sequences_pattern))\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for pred_file in predictions_files:\n",
    "        try:\n",
    "            df = pd.read_csv(pred_file, sep='\\t')\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not read predictions file '{pred_file}'. Error: {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "    for seq_file in sequences_files:\n",
    "        try:\n",
    "            df = pd.read_csv(seq_file, sep='\\t')\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not read sequences file '{seq_file}'. Error: {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "    if not df_list:\n",
    "        print(\"Warning: No output files were found to concatenate.\")\n",
    "        concatenated_df = pd.DataFrame(\n",
    "            columns=['ID', 'dataset', 'label_positive_probability', 'junction_aa', 'v_call', 'j_call'])\n",
    "    else:\n",
    "        concatenated_df = pd.concat(df_list, ignore_index=True)\n",
    "    submissions_file = os.path.join(out_dir, 'submissions.csv')\n",
    "    concatenated_df.to_csv(submissions_file, index=False)\n",
    "    print(f\"Concatenated output written to `{submissions_file}`.\")\n",
    "\n",
    "\n",
    "def get_dataset_pairs(train_dir: str, test_dir: str) -> List[Tuple[str, List[str]]]:\n",
    "    \"\"\"Returns list of (train_path, [test_paths]) tuples for dataset pairs.\"\"\"\n",
    "    test_groups = defaultdict(list)\n",
    "    for test_name in sorted(os.listdir(test_dir)):\n",
    "        if test_name.startswith(\"test_dataset_\"):\n",
    "            base_id = test_name.replace(\"test_dataset_\", \"\").split(\"_\")[0]\n",
    "            test_groups[base_id].append(os.path.join(test_dir, test_name))\n",
    "\n",
    "    pairs = []\n",
    "    for train_name in sorted(os.listdir(train_dir)):\n",
    "        if train_name.startswith(\"train_dataset_\"):\n",
    "            train_id = train_name.replace(\"train_dataset_\", \"\")\n",
    "            train_path = os.path.join(train_dir, train_name)\n",
    "            pairs.append((train_path, test_groups.get(train_id, [])))\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "This section explores the immune repertoire dataset to understand:\n",
    "1. **Data structure and quality** - File structure, missing values, data types\n",
    "2. **Sequence characteristics** - CDR3 length distribution, amino acid composition\n",
    "3. **Label distribution** - Class balance across datasets\n",
    "4. **Gene usage patterns** - V/J gene frequencies\n",
    "5. **Positive vs Negative comparison** - Differences in repertoire characteristics\n",
    "6. **Cross-dataset analysis** - Variability across multiple training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview - Explore Available Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "train_datasets_dir = \"/kaggle/input/adaptive-immune-profiling-challenge-2025/train_datasets/train_datasets\"\n",
    "\n",
    "# Get all training dataset directories\n",
    "train_dirs = sorted([d for d in os.listdir(train_datasets_dir) if d.startswith('train_dataset_')])\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"DATASET OVERVIEW\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total training datasets found: {len(train_dirs)}\\n\")\n",
    "\n",
    "# Collect high-level statistics for all datasets\n",
    "dataset_summary = []\n",
    "\n",
    "for train_dir_name in train_dirs:\n",
    "    train_path = os.path.join(train_datasets_dir, train_dir_name)\n",
    "    metadata_path = os.path.join(train_path, 'metadata.csv')\n",
    "    \n",
    "    if os.path.exists(metadata_path):\n",
    "        metadata = pd.read_csv(metadata_path)\n",
    "        \n",
    "        # Get TSV file sizes\n",
    "        tsv_files = glob.glob(os.path.join(train_path, '*.tsv'))\n",
    "        total_size_mb = sum(os.path.getsize(f) for f in tsv_files) / (1024 * 1024)\n",
    "        \n",
    "        dataset_summary.append({\n",
    "            'Dataset': train_dir_name,\n",
    "            'N_Repertoires': len(metadata),\n",
    "            'N_Positive': int(metadata['label_positive'].sum()),\n",
    "            'N_Negative': int((~metadata['label_positive']).sum()),\n",
    "            'Class_Balance (%)': round(metadata['label_positive'].mean() * 100, 2),\n",
    "            'Total_Size_MB': round(total_size_mb, 2)\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(dataset_summary)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"AGGREGATE STATISTICS ACROSS ALL DATASETS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total repertoires: {summary_df['N_Repertoires'].sum()}\")\n",
    "print(f\"Total positive samples: {summary_df['N_Positive'].sum()}\")\n",
    "print(f\"Total negative samples: {summary_df['N_Negative'].sum()}\")\n",
    "print(f\"Average class balance: {summary_df['Class_Balance (%)'].mean():.2f}%\")\n",
    "print(f\"Total data size: {summary_df['Total_Size_MB'].sum():.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset statistics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Number of repertoires per dataset\n",
    "axes[0].bar(range(len(summary_df)), summary_df['N_Repertoires'], color='steelblue', alpha=0.7)\n",
    "axes[0].set_xlabel('Dataset Index')\n",
    "axes[0].set_ylabel('Number of Repertoires')\n",
    "axes[0].set_title('Repertoires per Dataset')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Class balance distribution\n",
    "axes[1].bar(range(len(summary_df)), summary_df['Class_Balance (%)'], color='coral', alpha=0.7)\n",
    "axes[1].axhline(y=50, color='red', linestyle='--', linewidth=2, label='50% balance')\n",
    "axes[1].set_xlabel('Dataset Index')\n",
    "axes[1].set_ylabel('Positive Class (%)')\n",
    "axes[1].set_title('Class Balance per Dataset')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Dataset sizes\n",
    "axes[2].bar(range(len(summary_df)), summary_df['Total_Size_MB'], color='seagreen', alpha=0.7)\n",
    "axes[2].set_xlabel('Dataset Index')\n",
    "axes[2].set_ylabel('Size (MB)')\n",
    "axes[2].set_title('Dataset Size per Training Set')\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep Dive: Single Dataset Analysis\n",
    "Let's explore one representative dataset in detail to understand the data structure and sequence characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first training dataset for detailed analysis\n",
    "sample_train_dir = os.path.join(train_datasets_dir, train_dirs[0])\n",
    "print(f\"Analyzing: {train_dirs[0]}\\n\")\n",
    "\n",
    "# Load metadata\n",
    "metadata_df = pd.read_csv(os.path.join(sample_train_dir, 'metadata.csv'))\n",
    "print(f\"{'='*80}\")\n",
    "print(\"METADATA STRUCTURE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Shape: {metadata_df.shape}\")\n",
    "print(f\"\\nColumns: {metadata_df.columns.tolist()}\")\n",
    "print(f\"\\nData types:\\n{metadata_df.dtypes}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(metadata_df.head())\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"LABEL DISTRIBUTION\")\n",
    "print(f\"{'='*80}\")\n",
    "print(metadata_df['label_positive'].value_counts())\n",
    "print(f\"\\nPositive class percentage: {metadata_df['label_positive'].mean():.2%}\")\n",
    "\n",
    "# Load a sample repertoire file\n",
    "sample_file = metadata_df['filename'].iloc[0]\n",
    "sample_repertoire = pd.read_csv(os.path.join(sample_train_dir, sample_file), sep='\\t')\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SAMPLE REPERTOIRE FILE: {sample_file}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Shape: {sample_repertoire.shape}\")\n",
    "print(f\"\\nColumns: {sample_repertoire.columns.tolist()}\")\n",
    "print(f\"\\nData types:\\n{sample_repertoire.dtypes}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(sample_repertoire.head(10))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MISSING VALUES IN SAMPLE REPERTOIRE\")\n",
    "print(f\"{'='*80}\")\n",
    "missing_counts = sample_repertoire.isnull().sum()\n",
    "missing_pct = (missing_counts / len(sample_repertoire) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Count': missing_counts, 'Percentage': missing_pct})\n",
    "print(missing_df[missing_df['Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sequence-Level Analysis Across All Datasets\n",
    "Analyze CDR3 junction sequences, lengths, and amino acid composition across all training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full dataset (combining all repertoires) from the sample dataset\n",
    "print(f\"Loading full dataset from {train_dirs[0]}...\")\n",
    "full_dataset = load_full_dataset(sample_train_dir)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FULL DATASET STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total sequences: {len(full_dataset):,}\")\n",
    "print(f\"Unique repertoires: {full_dataset['ID'].nunique()}\")\n",
    "print(f\"Unique CDR3 sequences: {full_dataset['junction_aa'].nunique():,}\")\n",
    "print(f\"Sequence redundancy: {len(full_dataset) / full_dataset['junction_aa'].nunique():.2f}x\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CDR3 SEQUENCE LENGTH ANALYSIS\")\n",
    "print(f\"{'='*80}\")\n",
    "seq_lengths = full_dataset['junction_aa'].str.len()\n",
    "print(seq_lengths.describe())\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"REPERTOIRE SIZE ANALYSIS\")\n",
    "print(f\"{'='*80}\")\n",
    "repertoire_sizes = full_dataset.groupby('ID').size()\n",
    "print(repertoire_sizes.describe())\n",
    "print(f\"\\nSmallest repertoire: {repertoire_sizes.min()} sequences\")\n",
    "print(f\"Largest repertoire: {repertoire_sizes.max()} sequences\")\n",
    "print(f\"Median repertoire size: {repertoire_sizes.median():.0f} sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sequence characteristics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Plot 1: CDR3 Length Distribution\n",
    "axes[0, 0].hist(seq_lengths, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('CDR3 Length (amino acids)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title(f'CDR3 Length Distribution\\n(Mean: {seq_lengths.mean():.1f}, Std: {seq_lengths.std():.1f})')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Repertoire Size Distribution\n",
    "axes[0, 1].hist(repertoire_sizes, bins=30, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Repertoire Size (# sequences)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title(f'Repertoire Size Distribution\\n(Median: {repertoire_sizes.median():.0f})')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Top V genes\n",
    "v_gene_counts = full_dataset['v_call'].value_counts().head(20)\n",
    "axes[0, 2].barh(range(len(v_gene_counts)), v_gene_counts.values, color='mediumseagreen', alpha=0.7)\n",
    "axes[0, 2].set_yticks(range(len(v_gene_counts)))\n",
    "axes[0, 2].set_yticklabels(v_gene_counts.index, fontsize=8)\n",
    "axes[0, 2].set_xlabel('Count')\n",
    "axes[0, 2].set_title('Top 20 V Genes')\n",
    "axes[0, 2].invert_yaxis()\n",
    "axes[0, 2].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 4: Top J genes\n",
    "j_gene_counts = full_dataset['j_call'].value_counts().head(20)\n",
    "axes[1, 0].barh(range(len(j_gene_counts)), j_gene_counts.values, color='mediumpurple', alpha=0.7)\n",
    "axes[1, 0].set_yticks(range(len(j_gene_counts)))\n",
    "axes[1, 0].set_yticklabels(j_gene_counts.index, fontsize=8)\n",
    "axes[1, 0].set_xlabel('Count')\n",
    "axes[1, 0].set_title('Top 20 J Genes')\n",
    "axes[1, 0].invert_yaxis()\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 5: Amino Acid Composition\n",
    "aa_sequence = ''.join(full_dataset['junction_aa'].dropna().astype(str))\n",
    "aa_counts = Counter(aa_sequence)\n",
    "aa_df = pd.DataFrame.from_dict(aa_counts, orient='index', columns=['Count']).sort_values('Count', ascending=False)\n",
    "aa_df['Percentage'] = (aa_df['Count'] / aa_df['Count'].sum() * 100)\n",
    "\n",
    "axes[1, 1].bar(range(len(aa_df)), aa_df['Percentage'].values, color='orange', alpha=0.7)\n",
    "axes[1, 1].set_xticks(range(len(aa_df)))\n",
    "axes[1, 1].set_xticklabels(aa_df.index, fontsize=10)\n",
    "axes[1, 1].set_xlabel('Amino Acid')\n",
    "axes[1, 1].set_ylabel('Percentage (%)')\n",
    "axes[1, 1].set_title('Amino Acid Composition in CDR3')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 6: Sequence Redundancy\n",
    "top_sequences = full_dataset['junction_aa'].value_counts().head(20)\n",
    "axes[1, 2].barh(range(len(top_sequences)), top_sequences.values, color='teal', alpha=0.7)\n",
    "axes[1, 2].set_yticks(range(len(top_sequences)))\n",
    "axes[1, 2].set_yticklabels([seq[:15] + '...' if len(seq) > 15 else seq for seq in top_sequences.index], fontsize=8)\n",
    "axes[1, 2].set_xlabel('Occurrence Count')\n",
    "axes[1, 2].set_title('Top 20 Most Common CDR3 Sequences')\n",
    "axes[1, 2].invert_yaxis()\n",
    "axes[1, 2].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop 10 Amino Acids:\")\n",
    "print(aa_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Positive vs Negative Sample Comparison\n",
    "Compare characteristics between positive and negative labeled samples to identify potential discriminative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data by label\n",
    "pos_samples = full_dataset[full_dataset['label_positive'] == True]\n",
    "neg_samples = full_dataset[full_dataset['label_positive'] == False]\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"POSITIVE vs NEGATIVE SAMPLE COMPARISON\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nPositive samples:\")\n",
    "print(f\"  - Total sequences: {len(pos_samples):,}\")\n",
    "print(f\"  - From {pos_samples['ID'].nunique()} repertoires\")\n",
    "print(f\"  - Unique CDR3 sequences: {pos_samples['junction_aa'].nunique():,}\")\n",
    "\n",
    "print(f\"\\nNegative samples:\")\n",
    "print(f\"  - Total sequences: {len(neg_samples):,}\")\n",
    "print(f\"  - From {neg_samples['ID'].nunique()} repertoires\")\n",
    "print(f\"  - Unique CDR3 sequences: {neg_samples['junction_aa'].nunique():,}\")\n",
    "\n",
    "# Repertoire size comparison\n",
    "pos_sizes = pos_samples.groupby('ID').size()\n",
    "neg_sizes = neg_samples.groupby('ID').size()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"REPERTOIRE SIZE COMPARISON\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nPositive repertoires:\")\n",
    "print(pos_sizes.describe())\n",
    "print(f\"\\nNegative repertoires:\")\n",
    "print(neg_sizes.describe())\n",
    "\n",
    "# Sequence length comparison\n",
    "pos_lengths = pos_samples['junction_aa'].str.len()\n",
    "neg_lengths = neg_samples['junction_aa'].str.len()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CDR3 LENGTH COMPARISON\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nPositive samples:\")\n",
    "print(pos_lengths.describe())\n",
    "print(f\"\\nNegative samples:\")\n",
    "print(neg_lengths.describe())\n",
    "\n",
    "# Statistical test\n",
    "from scipy import stats\n",
    "t_stat, p_value = stats.ttest_ind(pos_lengths.dropna(), neg_lengths.dropna())\n",
    "print(f\"\\nT-test for length difference: t={t_stat:.4f}, p-value={p_value:.4e}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"âœ“ Significant difference in CDR3 lengths between groups!\")\n",
    "else:\n",
    "    print(\"âœ— No significant difference in CDR3 lengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize positive vs negative comparisons\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Repertoire size comparison\n",
    "axes[0, 0].hist([pos_sizes, neg_sizes], bins=30, label=['Positive', 'Negative'], \n",
    "                color=['red', 'blue'], alpha=0.6, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Repertoire Size')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Repertoire Size Distribution')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: CDR3 length comparison\n",
    "axes[0, 1].hist([pos_lengths, neg_lengths], bins=50, label=['Positive', 'Negative'],\n",
    "                color=['red', 'blue'], alpha=0.6, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('CDR3 Length')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title(f'CDR3 Length Distribution\\n(p-value: {p_value:.4e})')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Top V genes comparison\n",
    "pos_v_genes = pos_samples['v_call'].value_counts().head(15)\n",
    "neg_v_genes = neg_samples['v_call'].value_counts().head(15)\n",
    "\n",
    "# Normalize to percentages\n",
    "pos_v_pct = (pos_v_genes / len(pos_samples) * 100)\n",
    "neg_v_pct = (neg_v_genes / len(neg_samples) * 100)\n",
    "\n",
    "# Get union of top genes\n",
    "all_genes = sorted(set(pos_v_pct.index) | set(neg_v_pct.index))\n",
    "x = np.arange(len(all_genes))\n",
    "width = 0.35\n",
    "\n",
    "pos_values = [pos_v_pct.get(g, 0) for g in all_genes]\n",
    "neg_values = [neg_v_pct.get(g, 0) for g in all_genes]\n",
    "\n",
    "axes[1, 0].barh(x - width/2, pos_values, width, label='Positive', color='red', alpha=0.6)\n",
    "axes[1, 0].barh(x + width/2, neg_values, width, label='Negative', color='blue', alpha=0.6)\n",
    "axes[1, 0].set_yticks(x)\n",
    "axes[1, 0].set_yticklabels(all_genes, fontsize=8)\n",
    "axes[1, 0].set_xlabel('Percentage (%)')\n",
    "axes[1, 0].set_title('Top V Gene Usage Comparison')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].invert_yaxis()\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 4: Top J genes comparison\n",
    "pos_j_genes = pos_samples['j_call'].value_counts().head(15)\n",
    "neg_j_genes = neg_samples['j_call'].value_counts().head(15)\n",
    "\n",
    "pos_j_pct = (pos_j_genes / len(pos_samples) * 100)\n",
    "neg_j_pct = (neg_j_genes / len(neg_samples) * 100)\n",
    "\n",
    "all_j_genes = sorted(set(pos_j_pct.index) | set(neg_j_pct.index))\n",
    "x = np.arange(len(all_j_genes))\n",
    "\n",
    "pos_j_values = [pos_j_pct.get(g, 0) for g in all_j_genes]\n",
    "neg_j_values = [neg_j_pct.get(g, 0) for g in all_j_genes]\n",
    "\n",
    "axes[1, 1].barh(x - width/2, pos_j_values, width, label='Positive', color='red', alpha=0.6)\n",
    "axes[1, 1].barh(x + width/2, neg_j_values, width, label='Negative', color='blue', alpha=0.6)\n",
    "axes[1, 1].set_yticks(x)\n",
    "axes[1, 1].set_yticklabels(all_j_genes, fontsize=8)\n",
    "axes[1, 1].set_xlabel('Percentage (%)')\n",
    "axes[1, 1].set_title('Top J Gene Usage Comparison')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].invert_yaxis()\n",
    "axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find genes with largest difference between positive and negative\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"V GENES WITH LARGEST DIFFERENCES (Positive - Negative)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "all_v_genes = set(pos_v_genes.index) | set(neg_v_genes.index)\n",
    "v_gene_diffs = []\n",
    "for gene in all_v_genes:\n",
    "    pos_pct = (pos_samples['v_call'] == gene).sum() / len(pos_samples) * 100\n",
    "    neg_pct = (neg_samples['v_call'] == gene).sum() / len(neg_samples) * 100\n",
    "    v_gene_diffs.append({\n",
    "        'Gene': gene,\n",
    "        'Pos%': pos_pct,\n",
    "        'Neg%': neg_pct,\n",
    "        'Diff': pos_pct - neg_pct\n",
    "    })\n",
    "\n",
    "v_diff_df = pd.DataFrame(v_gene_diffs).sort_values('Diff', ascending=False, key=abs)\n",
    "print(v_diff_df.tail(10).to_string(index=False))\n",
    "print(\"\\n\")\n",
    "print(v_diff_df.head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"J GENES WITH LARGEST DIFFERENCES (Positive - Negative)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "all_j_genes = set(pos_j_genes.index) | set(neg_j_genes.index)\n",
    "j_gene_diffs = []\n",
    "for gene in all_j_genes:\n",
    "    pos_pct = (pos_samples['j_call'] == gene).sum() / len(pos_samples) * 100\n",
    "    neg_pct = (neg_samples['j_call'] == gene).sum() / len(neg_samples) * 100\n",
    "    j_gene_diffs.append({\n",
    "        'Gene': gene,\n",
    "        'Pos%': pos_pct,\n",
    "        'Neg%': neg_pct,\n",
    "        'Diff': pos_pct - neg_pct\n",
    "    })\n",
    "\n",
    "j_diff_df = pd.DataFrame(j_gene_diffs).sort_values('Diff', ascending=False, key=abs)\n",
    "print(j_diff_df.tail(10).to_string(index=False))\n",
    "print(\"\\n\")\n",
    "print(j_diff_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Dataset Analysis\n",
    "Analyze variability and patterns across ALL training datasets to understand dataset-specific characteristics and potential batch effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze multiple datasets - sample first 5 for speed\n",
    "n_datasets_to_analyze = min(5, len(train_dirs))\n",
    "print(f\"Analyzing first {n_datasets_to_analyze} training datasets for cross-dataset comparison...\")\n",
    "\n",
    "cross_dataset_stats = []\n",
    "\n",
    "for train_dir_name in tqdm(train_dirs[:n_datasets_to_analyze], desc=\"Processing datasets\"):\n",
    "    train_path = os.path.join(train_datasets_dir, train_dir_name)\n",
    "    \n",
    "    # Load metadata\n",
    "    metadata = pd.read_csv(os.path.join(train_path, 'metadata.csv'))\n",
    "    \n",
    "    # Load sample repertoire to get sequence stats\n",
    "    sample_sizes = []\n",
    "    sample_lengths = []\n",
    "    \n",
    "    # Sample a few repertoires per dataset for efficiency\n",
    "    for filename in metadata['filename'].iloc[:min(10, len(metadata))]:\n",
    "        try:\n",
    "            rep_df = pd.read_csv(os.path.join(train_path, filename), sep='\\t')\n",
    "            sample_sizes.append(len(rep_df))\n",
    "            sample_lengths.extend(rep_df['junction_aa'].str.len().dropna().tolist())\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    cross_dataset_stats.append({\n",
    "        'Dataset': train_dir_name,\n",
    "        'N_Repertoires': len(metadata),\n",
    "        'Class_Balance': metadata['label_positive'].mean(),\n",
    "        'Avg_Repertoire_Size': np.mean(sample_sizes) if sample_sizes else 0,\n",
    "        'Avg_CDR3_Length': np.mean(sample_lengths) if sample_lengths else 0,\n",
    "        'Std_CDR3_Length': np.std(sample_lengths) if sample_lengths else 0\n",
    "    })\n",
    "\n",
    "cross_df = pd.DataFrame(cross_dataset_stats)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CROSS-DATASET STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(cross_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"VARIABILITY ACROSS DATASETS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Class balance range: {cross_df['Class_Balance'].min():.2%} - {cross_df['Class_Balance'].max():.2%}\")\n",
    "print(f\"Avg repertoire size range: {cross_df['Avg_Repertoire_Size'].min():.0f} - {cross_df['Avg_Repertoire_Size'].max():.0f}\")\n",
    "print(f\"Avg CDR3 length range: {cross_df['Avg_CDR3_Length'].min():.2f} - {cross_df['Avg_CDR3_Length'].max():.2f}\")\n",
    "print(f\"\\nCoefficient of variation:\")\n",
    "print(f\"  - Class balance: {cross_df['Class_Balance'].std() / cross_df['Class_Balance'].mean():.2%}\")\n",
    "print(f\"  - Repertoire size: {cross_df['Avg_Repertoire_Size'].std() / cross_df['Avg_Repertoire_Size'].mean():.2%}\")\n",
    "print(f\"  - CDR3 length: {cross_df['Avg_CDR3_Length'].std() / cross_df['Avg_CDR3_Length'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-dataset variability\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "dataset_indices = range(len(cross_df))\n",
    "\n",
    "# Plot 1: Class balance across datasets\n",
    "axes[0, 0].plot(dataset_indices, cross_df['Class_Balance'] * 100, marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "axes[0, 0].axhline(y=50, color='red', linestyle='--', linewidth=2, label='50% balance')\n",
    "axes[0, 0].fill_between(dataset_indices, \n",
    "                         cross_df['Class_Balance'] * 100 - 5, \n",
    "                         cross_df['Class_Balance'] * 100 + 5, \n",
    "                         alpha=0.2)\n",
    "axes[0, 0].set_xlabel('Dataset Index')\n",
    "axes[0, 0].set_ylabel('Positive Class (%)')\n",
    "axes[0, 0].set_title('Class Balance Across Datasets')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Average repertoire size\n",
    "axes[0, 1].bar(dataset_indices, cross_df['Avg_Repertoire_Size'], color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Dataset Index')\n",
    "axes[0, 1].set_ylabel('Average Repertoire Size')\n",
    "axes[0, 1].set_title('Average Repertoire Size per Dataset')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Average CDR3 length with error bars\n",
    "axes[1, 0].errorbar(dataset_indices, cross_df['Avg_CDR3_Length'], \n",
    "                    yerr=cross_df['Std_CDR3_Length'],\n",
    "                    marker='o', linewidth=2, markersize=8, capsize=5, color='seagreen')\n",
    "axes[1, 0].set_xlabel('Dataset Index')\n",
    "axes[1, 0].set_ylabel('Average CDR3 Length')\n",
    "axes[1, 0].set_title('Average CDR3 Length Across Datasets (with std dev)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Heatmap of normalized characteristics\n",
    "norm_df = cross_df[['Class_Balance', 'Avg_Repertoire_Size', 'Avg_CDR3_Length']].copy()\n",
    "# Normalize each column to 0-1 scale for comparison\n",
    "for col in norm_df.columns:\n",
    "    norm_df[col] = (norm_df[col] - norm_df[col].min()) / (norm_df[col].max() - norm_df[col].min())\n",
    "\n",
    "im = axes[1, 1].imshow(norm_df.T, cmap='YlOrRd', aspect='auto')\n",
    "axes[1, 1].set_xticks(dataset_indices)\n",
    "axes[1, 1].set_xticklabels([f\"D{i}\" for i in dataset_indices])\n",
    "axes[1, 1].set_yticks(range(len(norm_df.columns)))\n",
    "axes[1, 1].set_yticklabels(['Class Balance', 'Repertoire Size', 'CDR3 Length'])\n",
    "axes[1, 1].set_xlabel('Dataset')\n",
    "axes[1, 1].set_title('Normalized Dataset Characteristics Heatmap')\n",
    "plt.colorbar(im, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings Summary & Recommendations\n",
    "\n",
    "Based on the EDA, here are actionable insights for model development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'='*80}\")\n",
    "print(\"KEY FINDINGS & RECOMMENDATIONS FOR MODEL DEVELOPMENT\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "findings = [\n",
    "    (\"ðŸ“Š Data Structure\", [\n",
    "        f\"â€¢ Total datasets: {len(train_dirs)}\",\n",
    "        f\"â€¢ Total repertoires analyzed: {summary_df['N_Repertoires'].sum()}\",\n",
    "        f\"â€¢ Repertoire sizes vary widely: {repertoire_sizes.min():.0f} - {repertoire_sizes.max():.0f} sequences\",\n",
    "        \"â€¢ Consider: Normalize repertoire-level features by repertoire size\"\n",
    "    ]),\n",
    "    \n",
    "    (\"ðŸ§¬ Sequence Characteristics\", [\n",
    "        f\"â€¢ CDR3 length: Mean={seq_lengths.mean():.1f}, Std={seq_lengths.std():.1f}\",\n",
    "        f\"â€¢ High sequence redundancy: {len(full_dataset) / full_dataset['junction_aa'].nunique():.2f}x\",\n",
    "        f\"â€¢ Most common amino acids: {', '.join(aa_df.head(5).index.tolist())}\",\n",
    "        \"â€¢ Consider: Use k-mer features (3-5mers) to capture sequence patterns\"\n",
    "    ]),\n",
    "    \n",
    "    (\"âš–ï¸ Class Balance\", [\n",
    "        f\"â€¢ Average class balance: {summary_df['Class_Balance (%)'].mean():.2f}%\",\n",
    "        f\"â€¢ Range: {summary_df['Class_Balance (%)'].min():.2f}% - {summary_df['Class_Balance (%)'].max():.2f}%\",\n",
    "        \"â€¢ Recommendation: Use stratified cross-validation\",\n",
    "        \"â€¢ Consider: Class weighting or SMOTE if severe imbalance exists\"\n",
    "    ]),\n",
    "    \n",
    "    (\"ðŸ”¬ Positive vs Negative Differences\", [\n",
    "        f\"â€¢ Repertoire size difference: Pos={pos_sizes.mean():.0f}, Neg={neg_sizes.mean():.0f}\",\n",
    "        f\"â€¢ CDR3 length p-value: {p_value:.4e} ({'Significant' if p_value < 0.05 else 'Not significant'})\",\n",
    "        \"â€¢ V/J gene usage shows differences between groups\",\n",
    "        \"â€¢ Consider: Gene usage features may be highly informative\"\n",
    "    ]),\n",
    "    \n",
    "    (\"ðŸ”„ Cross-Dataset Variability\", [\n",
    "        f\"â€¢ Class balance CV: {cross_df['Class_Balance'].std() / cross_df['Class_Balance'].mean():.2%}\",\n",
    "        f\"â€¢ Repertoire size CV: {cross_df['Avg_Repertoire_Size'].std() / cross_df['Avg_Repertoire_Size'].mean():.2%}\",\n",
    "        \"â€¢ Potential batch effects exist across datasets\",\n",
    "        \"â€¢ Recommendation: Train separate models per dataset or use dataset ID as feature\"\n",
    "    ]),\n",
    "    \n",
    "    (\"ðŸ’¡ Feature Engineering Ideas\", [\n",
    "        \"1. K-mer frequencies (3-5mers) from CDR3 sequences\",\n",
    "        \"2. V/J gene usage patterns (one-hot or frequency encoding)\",\n",
    "        \"3. CDR3 length statistics (mean, std, distribution)\",\n",
    "        \"4. Amino acid composition features\",\n",
    "        \"5. Repertoire diversity metrics (Shannon entropy, clonality)\",\n",
    "        \"6. Sequence motif detection\",\n",
    "        \"7. V-J gene pairing patterns\"\n",
    "    ]),\n",
    "    \n",
    "    (\"ðŸŽ¯ Model Recommendations\", [\n",
    "        \"â€¢ Random Forest: Good baseline, provides feature importance\",\n",
    "        \"â€¢ Gradient Boosting (XGBoost/LightGBM): Better performance expected\",\n",
    "        \"â€¢ Neural Networks: For sequence embeddings (LSTM/Transformers)\",\n",
    "        \"â€¢ Ensemble Methods: Combine multiple approaches\",\n",
    "        \"â€¢ Cross-validation: Use per-dataset stratified splits\"\n",
    "    ])\n",
    "]\n",
    "\n",
    "for category, items in findings:\n",
    "    print(f\"\\n{category}\")\n",
    "    print(\"-\" * 80)\n",
    "    for item in items:\n",
    "        print(item)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"âœ“ EDA COMPLETE - Ready for model development!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Main ImmuneStatePredictor class, where participants will fill in their implementations within the placeholders \n",
    "## and replace any example code lines with actual code that makes sense\n",
    "\n",
    "\n",
    "class ImmuneStatePredictor:\n",
    "    \"\"\"\n",
    "    A template for predicting immune states from TCR repertoire data.\n",
    "\n",
    "    Participants should implement the logic for training, prediction, and\n",
    "    sequence identification within this class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_jobs: int = 1, device: str = 'cpu', **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the predictor.\n",
    "\n",
    "        Args:\n",
    "            n_jobs (int): Number of CPU cores to use for parallel processing.\n",
    "            device (str): The device to use for computation (e.g., 'cpu', 'cuda').\n",
    "            **kwargs: Additional hyperparameters for the model.\n",
    "        \"\"\"\n",
    "        total_cores = os.cpu_count()\n",
    "        if n_jobs == -1:\n",
    "            self.n_jobs = total_cores\n",
    "        else:\n",
    "            self.n_jobs = min(n_jobs, total_cores)\n",
    "        self.device = device\n",
    "        if device == 'cuda' and not torch.cuda.is_available():\n",
    "            print(\"Warning: 'cuda' was requested but is not available. Falling back to 'cpu'.\")\n",
    "            self.device = 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "        # --- your code starts here ---\n",
    "        # Example: Store hyperparameters, the actual model, identified important sequences, etc.\n",
    "\n",
    "        # NOTE: we encourage you to use self.n_jobs and self.device if appropriate in\n",
    "        # your implementation instead of hardcoding these values because your code may later be run in an\n",
    "        # environment with different hardware resources.\n",
    "\n",
    "        self.model = None\n",
    "        self.important_sequences_ = None\n",
    "        # --- your code ends here ---\n",
    "\n",
    "    def fit(self, train_dir_path: str):\n",
    "        \"\"\"\n",
    "        Trains the model on the provided training data.\n",
    "\n",
    "        Args:\n",
    "            train_dir_path (str): Path to the directory with training TSV files.\n",
    "\n",
    "        Returns:\n",
    "            self: The fitted predictor instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # --- your code starts here ---\n",
    "        # Load the data, prepare suited representations as needed, train your model,\n",
    "        # and find the top k important sequences that best explain the labels.\n",
    "        # Example: Load the data. One possibility could be to use the provided utility function as shown below.\n",
    "\n",
    "        # full_train_dataset_df = load_full_dataset(train_d\n",
    "        \n",
    "                # STEP 1: Load metadata and get list of repertoire files\n",
    "        print(f'Loading training data from {train_dir_path}')\n",
    "        metadata_path = os.path.join(train_dir_path, 'metadata.csv')\n",
    "        metadata_df = pd.read_csv(metadata_path)\n",
    "        \n",
    "        # Store training metadata for later use in identifying sequences\n",
    "        self.train_metadata_ = metadata_df\n",
    "        self.train_dir_path_ = train_dir_path\n",
    "        \n",
    "        print(f'Found {len(metadata_df)} repertoires')\n",
    "        print(f'Label distribution: {metadata_df[\"label_positive\"].value_counts().to_dict()}')\n",
    "        \n",
    "        # STEP 2: Feature engineering - create repertoire-level features\n",
    "        print('Extracting features from repertoires...')\n",
    "        \n",
    "        # Extract k-mer features and count-based features from each repertoire\n",
    "        from collections import Counter\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        \n",
    "        # Collect all sequences from all repertoires to build vocabulary\n",
    "        all_sequences = []\n",
    "        repertoire_sequences = {}  # Store sequences per repertoire\n",
    "        repertoire_features = []  # Store feature vectors\n",
    "        labels = []\n",
    "        \n",
    "        for idx, row in metadata_df.iterrows():\n",
    "            repertoire_id = row['repertoire_id']\n",
    "            filename = row['filename']\n",
    "            label = row['label_positive']\n",
    "            \n",
    "            # Load repertoire file\n",
    "            repertoire_path = os.path.join(train_dir_path, filename)\n",
    "            repertoire_df = pd.read_csv(repertoire_path, sep='\\t')\n",
    "            \n",
    "            # Store sequences for this repertoire\n",
    "            sequences = repertoire_df[['junction_aa', 'v_call', 'j_call']].copy()\n",
    "            repertoire_sequences[repertoire_id] = sequences\n",
    "            \n",
    "            labels.append(label)\n",
    "            \n",
    "        print(f'Loaded {len(repertoire_sequences)} repertoires')\n",
    "        \n",
    "        # STEP 3: Create sequence vocabulary and count features\n",
    "        # Build a global set of all unique (junction_aa, v_call, j_call) triplets\n",
    "        print('Building sequence vocabulary...')\n",
    "        sequence_vocab = set()\n",
    "        for rep_id, seqs in repertoire_sequences.items():\n",
    "            for _, row in seqs.iterrows():\n",
    "                triplet = (row['junction_aa'], row['v_call'], row['j_call'])\n",
    "                sequence_vocab.add(triplet)\n",
    "        \n",
    "        print(f'Total unique sequences across all repertoires: {len(sequence_vocab)}')\n",
    "        \n",
    "        # Convert to list for indexing\n",
    "        sequence_vocab = list(sequence_vocab)\n",
    "        sequence_to_idx = {seq: idx for idx, seq in enumerate(sequence_vocab)}\n",
    "        \n",
    "        # STEP 4: Create feature matrix (repertoire x sequence presence/count)\n",
    "        print('Creating feature matrix...')\n",
    "        import numpy as np\n",
    "        from scipy.sparse import lil_matrix\n",
    "        \n",
    "        n_repertoires = len(repertoire_sequences)\n",
    "        n_features = len(sequence_vocab)\n",
    "        \n",
    "        # Use sparse matrix for memory efficiency\n",
    "        X = lil_matrix((n_repertoires, n_features), dtype=np.float32)\n",
    "        \n",
    "        repertoire_ids = list(repertoire_sequences.keys())\n",
    "        for rep_idx, rep_id in enumerate(repertoire_ids):\n",
    "            seqs = repertoire_sequences[rep_id]\n",
    "            for _, row in seqs.iterrows():\n",
    "                triplet = (row['junction_aa'], row['v_call'], row['j_call'])\n",
    "                seq_idx = sequence_to_idx[triplet]\n",
    "                X[rep_idx, seq_idx] += 1  # Count occurrences\n",
    "        \n",
    "        # Convert to CSR format for efficient operations\n",
    "        X = X.tocsr()\n",
    "        \n",
    "        # Store for later use\n",
    "        self.sequence_vocab_ = sequence_vocab\n",
    "        self.sequence_to_idx_ = sequence_to_idx\n",
    "        self.repertoire_sequences_ = repertoire_sequences\n",
    "        \n",
    "        y = np.array(labels)\n",
    "        \n",
    "        print(f'Feature matrix shape: {X.shape}')\n",
    "        print(f'Number of non-zero elements: {X.nnz}')\n",
    "        \n",
    "        # STEP 5: Train a classifier\n",
    "        print('Training classifier...')\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        \n",
    "        # Use Random Forest for feature importance extraction\n",
    "        # You can experiment with other models like LightGBM, XGBoost, etc.\n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=42,\n",
    "            n_jobs=self.n_jobs,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        self.model.fit(X, y)\n",
    "        \n",
    "        print('Model training complete.')\n",
    "        \n",
    "        # STEP 6: Identify important sequences\n",
    "        print('Identifying important sequences...')\n",
    "        self.important_sequences_ = self.identify_associated_sequences(os.path.basename(train_dir_path))ir_path\n",
    "\n",
    "        #   Model Training\n",
    "        #    Example: self.model = SomeClassifier().fit(X_train, y_train)\n",
    "        self.model = \"some trained model\"  # Replace with your actual learnt model\n",
    "\n",
    "        #   Identify important sequences (can be done here or in the dedicated method)\n",
    "        #    Example:\n",
    "        self.important_sequences_ = self.identify_associated_sequences(top_k=50000, dataset_name=os.path.basename(train_dir_path))\n",
    "\n",
    "        # --- your code ends here ---\n",
    "        print(\"Training complete.\")\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, test_dir_path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Predicts probabilities for examples in the provided path.\n",
    "\n",
    "        Args:\n",
    "            test_dir_path (str): Path to the directory with test TSV files.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame with 'ID', 'dataset', 'label_positive_probability', 'junction_aa', 'v_call', 'j_call' columns.\n",
    "        \"\"\"\n",
    "        print(f\"Making predictions for data in {test_dir_path}...\")\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"The model has not been fitted yet. Please call `fit` first.\")\n",
    "\n",
    "        # --- your code starts here ---\n",
    "\n",
    "        # Example: Load the data. One possibility could be to use the provided utility function as shown below.\n",
    "\n",
    "        # full_test_dataset_df = load_full_dataset(test_dir_path)\n",
    "        repertoire_ids = get_repertoire_ids(test_dir_path)  # R\n",
    "        \n",
    "        # Load metadata for test set\n",
    "        metadata_path = os.path.join(test_dir_path, 'metadata.csv')\n",
    "        metadata_df = pd.read_csv(metadata_path)\n",
    "        \n",
    "        print(f'Predicting on {len(metadata_df)} test repertoires...')\n",
    "        \n",
    "        # Load and transform test repertoires to feature vectors\n",
    "        test_repertoire_sequences = {}\n",
    "        for idx, row in metadata_df.iterrows():\n",
    "            repertoire_id = row['repertoire_id']\n",
    "            filename = row['filename']\n",
    "            \n",
    "            # Load repertoire file\n",
    "            repertoire_path = os.path.join(test_dir_path, filename)\n",
    "            repertoire_df = pd.read_csv(repertoire_path, sep='\\t')\n",
    "            \n",
    "            # Store sequences for this repertoire\n",
    "            sequences = repertoire_df[['junction_aa', 'v_call', 'j_call']].copy()\n",
    "            test_repertoire_sequences[repertoire_id] = sequences\n",
    "        \n",
    "        # Transform to feature matrix using the same vocabulary from training\n",
    "        import numpy as np\n",
    "        from scipy.sparse import lil_matrix\n",
    "        \n",
    "        n_test_repertoires = len(test_repertoire_sequences)\n",
    "        n_features = len(self.sequence_vocab_)\n",
    "        \n",
    "        X_test = lil_matrix((n_test_repertoires, n_features), dtype=np.float32)\n",
    "        \n",
    "        test_repertoire_ids = list(test_repertoire_sequences.keys())\n",
    "        for rep_idx, rep_id in enumerate(test_repertoire_ids):\n",
    "            seqs = test_repertoire_sequences[rep_id]\n",
    "            for _, row in seqs.iterrows():\n",
    "                triplet = (row['junction_aa'], row['v_call'], row['j_call'])\n",
    "                # Only use sequences seen in training\n",
    "                if triplet in self.sequence_to_idx_:\n",
    "                    seq_idx = self.sequence_to_idx_[triplet]\n",
    "                    X_test[rep_idx, seq_idx] += 1\n",
    "        \n",
    "        X_test = X_test.tocsr()\n",
    "        \n",
    "        # Predict probabilities\n",
    "        probabilities = self.model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "        \n",
    "        print(f'Predictions complete.')eplace with actual repertoire IDs from the test data\n",
    "\n",
    "        # Prediction\n",
    "        #    Example:\n",
    "        # draw random probabilities for demonstration purposes\n",
    "\n",
    "        probabilities = np.random.rand(len(repertoire_ids)) # Replace with true predicted probabilities from your model\n",
    "\n",
    "        # --- your code ends here ---\n",
    "\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'ID': repertoire_ids,\n",
    "            'dataset': [os.path.basename(test_dir_path)] * len(repertoire_ids),\n",
    "            'label_positive_probability': probabilities\n",
    "        })\n",
    "\n",
    "        # to enable compatibility with the expected output format that includes junction_aa, v_call, j_call columns\n",
    "        predictions_df['junction_aa'] = -999.0\n",
    "        predictions_df['v_call'] = -999.0\n",
    "        predictions_df['j_call'] = -999.0\n",
    "\n",
    "        predictions_df = predictions_df[['ID', 'dataset', 'label_positive_probability', 'junction_aa', 'v_call', 'j_call']]\n",
    "\n",
    "        print(f\"Prediction complete on {len(repertoire_ids)} examples in {test_dir_path}.\")\n",
    "        return predictions_df\n",
    "\n",
    "    def identify_associated_sequences(self, dataset_name: str, top_k: int = 50000) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Identifies the top \"k\" important sequences (rows) from the training data that best explain the labels.\n",
    "\n",
    "        Args:\n",
    "            top_k (int): The number of top sequences to return (based on some scoring mechanism).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame with 'ID', 'dataset', 'label_positive_probability', 'junction_aa', 'v_call', 'j_call' columns.\n",
    "        \"\"\"\n",
    "\n",
    "        # --- your code starts here ---\n",
    "        \n",
    "        # Return the top k sequences, sorted based on some form of importance score.\n",
    "        # Example:\n",
    "        # all_sequences_scored = self._score_all_sequences()\n",
    "        \n",
    "        all_sequences_scored = generate_random_top_sequences_df\n",
    "        \n",
    "        # Extract feature importances from the trained model\n",
    "        print(f'Extracting feature importances for {dataset_name}...')\n",
    "        \n",
    "        # Get feature importances from Random Forest\n",
    "        feature_importances = self.model.feature_importances_\n",
    "        \n",
    "        # Create a DataFrame with sequences and their importance scores\n",
    "        importance_data = []\n",
    "        for seq_idx, importance_score in enumerate(feature_importances):\n",
    "            if importance_score > 0:  # Only include sequences with non-zero importance\n",
    "                triplet = self.sequence_vocab_[seq_idx]\n",
    "                junction_aa, v_call, j_call = triplet\n",
    "                importance_data.append({\n",
    "                    'junction_aa': junction_aa,\n",
    "                    'v_call': v_call,\n",
    "                    'j_call': j_call,\n",
    "                    'importance_score': importance_score\n",
    "                })\n",
    "        \n",
    "        all_sequences_scored = pd.DataFrame(importance_data)\n",
    "        \n",
    "        # Sort by importance score (descending)\n",
    "        all_sequences_scored = all_sequences_scored.sort_values(\n",
    "            by='importance_score', \n",
    "            ascending=False\n",
    "        ).reset_index(drop=True)\n",
    "        \n",
    "        print(f'Found {len(all_sequences_scored)} sequences with non-zero importance')\n",
    "        print(f'Top importance score: {all_sequences_scored[\"importance_score\"].iloc[0]:.6f}')\n",
    "        print(f'Mean importance score: {all_sequences_scored[\"importance_score\"].mean():.6f}')(n_seq=top_k)  # Replace with your way of identifying top k sequences\n",
    "\n",
    "        # note that all_sequences_scored should contain a 'importance_score' column that will be used further below\n",
    "        \n",
    "        # --- your code ends here ---\n",
    "\n",
    "        top_sequences_df = all_sequences_scored.nlargest(top_k, 'importance_score')\n",
    "        top_sequences_df = top_sequences_df[['junction_aa', 'v_call', 'j_call']]\n",
    "        top_sequences_df['dataset'] = dataset_name\n",
    "        top_sequences_df['ID'] = range(1, len(top_sequences_df)+1)\n",
    "        top_sequences_df['ID'] = top_sequences_df['dataset'] + '_seq_top_' + top_sequences_df['ID'].astype(str)\n",
    "        top_sequences_df['label_positive_probability'] = -999.0 # to enable compatibility with the expected output format\n",
    "        top_sequences_df = top_sequences_df[['ID', 'dataset', 'label_positive_probability', 'junction_aa', 'v_call', 'j_call']]\n",
    "\n",
    "        return top_sequences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## The `main` workflow that uses your implementation of the ImmuneStatePredictor class to train, identify important sequences and predict test labels\n",
    "\n",
    "\n",
    "def _train_predictor(predictor: ImmuneStatePredictor, train_dir: str):\n",
    "    \"\"\"Trains the predictor on the training data.\"\"\"\n",
    "    print(f\"Fitting model on examples in ` {train_dir} `...\")\n",
    "    predictor.fit(train_dir)\n",
    "\n",
    "\n",
    "def _generate_predictions(predictor: ImmuneStatePredictor, test_dirs: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Generates predictions for all test directories and concatenates them.\"\"\"\n",
    "    all_preds = []\n",
    "    for test_dir in test_dirs:\n",
    "        print(f\"Predicting on examples in ` {test_dir} `...\")\n",
    "        preds = predictor.predict_proba(test_dir)\n",
    "        if preds is not None and not preds.empty:\n",
    "            all_preds.append(preds)\n",
    "        else:\n",
    "            print(f\"Warning: No predictions returned for {test_dir}\")\n",
    "    if all_preds:\n",
    "        return pd.concat(all_preds, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def _save_predictions(predictions: pd.DataFrame, out_dir: str, train_dir: str) -> None:\n",
    "    \"\"\"Saves predictions to a TSV file.\"\"\"\n",
    "    if predictions.empty:\n",
    "        raise ValueError(\"No predictions to save - predictions DataFrame is empty\")\n",
    "\n",
    "    preds_path = os.path.join(out_dir, f\"{os.path.basename(train_dir)}_test_predictions.tsv\")\n",
    "    save_tsv(predictions, preds_path)\n",
    "    print(f\"Predictions written to `{preds_path}`.\")\n",
    "\n",
    "\n",
    "def _save_important_sequences(predictor: ImmuneStatePredictor, out_dir: str, train_dir: str) -> None:\n",
    "    \"\"\"Saves important sequences to a TSV file.\"\"\"\n",
    "    seqs = predictor.important_sequences_\n",
    "    if seqs is None or seqs.empty:\n",
    "        raise ValueError(\"No important sequences available to save\")\n",
    "\n",
    "    seqs_path = os.path.join(out_dir, f\"{os.path.basename(train_dir)}_important_sequences.tsv\")\n",
    "    save_tsv(seqs, seqs_path)\n",
    "    print(f\"Important sequences written to `{seqs_path}`.\")\n",
    "\n",
    "\n",
    "def main(train_dir: str, test_dirs: List[str], out_dir: str, n_jobs: int, device: str) -> None:\n",
    "    validate_dirs_and_files(train_dir, test_dirs, out_dir)\n",
    "    predictor = ImmuneStatePredictor(n_jobs=n_jobs,\n",
    "                                     device=device)  # instantiate with any other parameters as defined by you in the class\n",
    "    _train_predictor(predictor, train_dir)\n",
    "    predictions = _generate_predictions(predictor, test_dirs)\n",
    "    _save_predictions(predictions, out_dir, train_dir)\n",
    "    _save_important_sequences(predictor, out_dir, train_dir)\n",
    "\n",
    "\n",
    "def run():\n",
    "    parser = argparse.ArgumentParser(description=\"Immune State Predictor CLI\")\n",
    "    parser.add_argument(\"--train_dir\", required=True, help=\"Path to training data directory\")\n",
    "    parser.add_argument(\"--test_dirs\", required=True, nargs=\"+\", help=\"Path(s) to test data director(ies)\")\n",
    "    parser.add_argument(\"--out_dir\", required=True, help=\"Path to output directory\")\n",
    "    parser.add_argument(\"--n_jobs\", type=int, default=1,\n",
    "                        help=\"Number of CPU cores to use. Use -1 for all available cores.\")\n",
    "    parser.add_argument(\"--device\", type=str, default='cpu', choices=['cpu', 'cuda'],\n",
    "                        help=\"Device to use for computation ('cpu' or 'cuda').\")\n",
    "    args = parser.parse_args()\n",
    "    main(args.train_dir, args.test_dirs, args.out_dir, args.n_jobs, args.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_datasets_dir = \"/kaggle/input/adaptive-immune-profiling-challenge-2025/train_datasets/train_datasets\"\n",
    "test_datasets_dir = \"/kaggle/input/adaptive-immune-profiling-challenge-2025/test_datasets/test_datasets\"\n",
    "results_dir = \"/kaggle/working/results\"\n",
    "\n",
    "train_test_dataset_pairs = get_dataset_pairs(train_datasets_dir, test_datasets_dir)\n",
    "\n",
    "for train_dir, test_dirs in train_test_dataset_pairs:\n",
    "    main(train_dir=train_dir, test_dirs=test_dirs, out_dir=results_dir, n_jobs=4, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "concatenate_output_files(out_dir=results_dir)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13374319,
     "sourceId": 106680,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
