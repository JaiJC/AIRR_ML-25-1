{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIRR-ML-25: Adaptive Immune Profiling Challenge - EDA & Data Analysis\n",
    "\n",
    "## Challenge Overview\n",
    "**Objective:** Build ML models for two tasks:\n",
    "1. **Predict immune state** (disease vs. healthy) from adaptive immune repertoires\n",
    "2. **Identify immune receptor sequences** most strongly associated with the target immune state\n",
    "\n",
    "## Notebook Structure\n",
    "1. **Setup & Data Loading**\n",
    "2. **Metadata Analysis** - Labels, demographics, class balance\n",
    "3. **Sequence-Level Analysis** - junction_aa, v_call, j_call distributions\n",
    "4. **Diversity Metrics** - Uniqueness, entropy, richness\n",
    "5. **Technical Bias Check** - Batch effects across sequencing runs\n",
    "6. **HLA Gene Analysis** - Allele prevalence and label associations\n",
    "7. **Missing Values Assessment**\n",
    "8. **Train vs Test Comparison**\n",
    "9. **Dimensionality Reduction Visualization**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Iterator, Tuple, Union, List, Optional, Dict\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA PATHS CONFIGURATION\n",
    "# =============================================================================\n",
    "# Update these paths based on your environment (Kaggle vs Local)\n",
    "\n",
    "# For Kaggle:\n",
    "TRAIN_DIR = \"/kaggle/input/adaptive-immune-profiling-challenge-2025/train_datasets/train_datasets\"\n",
    "TEST_DIR = \"/kaggle/input/adaptive-immune-profiling-challenge-2025/test_datasets/test_datasets\"\n",
    "\n",
    "# For local testing (uncomment and modify if running locally):\n",
    "# TRAIN_DIR = \"/path/to/your/local/train_datasets\"\n",
    "# TEST_DIR = \"/path/to/your/local/test_datasets\"\n",
    "\n",
    "# Check if running on Kaggle\n",
    "IS_KAGGLE = os.path.exists(\"/kaggle/input\")\n",
    "print(f\"Running on Kaggle: {IS_KAGGLE}\")\n",
    "\n",
    "# Verify paths exist\n",
    "if IS_KAGGLE:\n",
    "    if os.path.exists(TRAIN_DIR):\n",
    "        train_datasets = sorted([d for d in os.listdir(TRAIN_DIR) if d.startswith(\"train_dataset_\")])\n",
    "        test_datasets = sorted([d for d in os.listdir(TEST_DIR) if d.startswith(\"test_dataset_\")])\n",
    "        print(f\"\\nüìÅ Found {len(train_datasets)} training datasets\")\n",
    "        print(f\"üìÅ Found {len(test_datasets)} test datasets\")\n",
    "        print(f\"\\nüîπ Training datasets: {train_datasets[:5]}{'...' if len(train_datasets) > 5 else ''}\")\n",
    "        print(f\"üîπ Test datasets: {test_datasets[:5]}{'...' if len(test_datasets) > 5 else ''}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Data directory not found. Please check the path.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not running on Kaggle. Update paths in the cell above for local execution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility Functions for Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def load_metadata(dataset_dir: str, metadata_filename: str = 'metadata.csv') -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load metadata.csv from a dataset directory.\"\"\"\n",
    "    metadata_path = os.path.join(dataset_dir, metadata_filename)\n",
    "    if os.path.exists(metadata_path):\n",
    "        return pd.read_csv(metadata_path)\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_all_metadata(base_dir: str, dataset_prefix: str = \"train_dataset_\") -> pd.DataFrame:\n",
    "    \"\"\"Load and combine metadata from all datasets in a directory.\"\"\"\n",
    "    all_metadata = []\n",
    "    datasets = sorted([d for d in os.listdir(base_dir) if d.startswith(dataset_prefix)])\n",
    "    \n",
    "    for dataset_name in tqdm(datasets, desc=\"Loading metadata\"):\n",
    "        dataset_path = os.path.join(base_dir, dataset_name)\n",
    "        metadata = load_metadata(dataset_path)\n",
    "        if metadata is not None:\n",
    "            metadata['dataset'] = dataset_name\n",
    "            all_metadata.append(metadata)\n",
    "    \n",
    "    if all_metadata:\n",
    "        return pd.concat(all_metadata, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def load_repertoire(file_path: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load a single repertoire TSV file.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(file_path, sep='\\t')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_sample_repertoires(base_dir: str, dataset_name: str, n_samples: int = 5) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Load a sample of repertoire files from a dataset for quick analysis.\"\"\"\n",
    "    dataset_path = os.path.join(base_dir, dataset_name)\n",
    "    tsv_files = glob.glob(os.path.join(dataset_path, \"*.tsv\"))[:n_samples]\n",
    "    \n",
    "    repertoires = {}\n",
    "    for file_path in tsv_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        repertoires[filename] = load_repertoire(file_path)\n",
    "    \n",
    "    return repertoires\n",
    "\n",
    "\n",
    "def get_repertoire_summary(dataset_path: str, sample_size: int = None) -> pd.DataFrame:\n",
    "    \"\"\"Get summary statistics for repertoires in a dataset.\"\"\"\n",
    "    metadata_path = os.path.join(dataset_path, 'metadata.csv')\n",
    "    summaries = []\n",
    "    \n",
    "    if os.path.exists(metadata_path):\n",
    "        metadata = pd.read_csv(metadata_path)\n",
    "        files_to_process = metadata['filename'].tolist()\n",
    "        if sample_size:\n",
    "            files_to_process = files_to_process[:sample_size]\n",
    "    else:\n",
    "        files_to_process = glob.glob(os.path.join(dataset_path, \"*.tsv\"))\n",
    "        if sample_size:\n",
    "            files_to_process = files_to_process[:sample_size]\n",
    "    \n",
    "    for filename in tqdm(files_to_process, desc=f\"Analyzing {os.path.basename(dataset_path)}\", leave=False):\n",
    "        if os.path.exists(metadata_path):\n",
    "            file_path = os.path.join(dataset_path, filename)\n",
    "        else:\n",
    "            file_path = filename\n",
    "            filename = os.path.basename(filename)\n",
    "        \n",
    "        repertoire = load_repertoire(file_path)\n",
    "        if repertoire is not None:\n",
    "            summary = {\n",
    "                'filename': filename,\n",
    "                'n_sequences': len(repertoire),\n",
    "                'n_unique_junction_aa': repertoire['junction_aa'].nunique() if 'junction_aa' in repertoire.columns else 0,\n",
    "                'n_unique_v_call': repertoire['v_call'].nunique() if 'v_call' in repertoire.columns else 0,\n",
    "                'n_unique_j_call': repertoire['j_call'].nunique() if 'j_call' in repertoire.columns else 0,\n",
    "                'has_d_call': 'd_call' in repertoire.columns,\n",
    "                'has_templates': 'templates' in repertoire.columns or 'duplicate_count' in repertoire.columns,\n",
    "                'columns': list(repertoire.columns)\n",
    "            }\n",
    "            \n",
    "            if 'junction_aa' in repertoire.columns:\n",
    "                seq_lengths = repertoire['junction_aa'].dropna().str.len()\n",
    "                summary['mean_seq_length'] = seq_lengths.mean()\n",
    "                summary['min_seq_length'] = seq_lengths.min()\n",
    "                summary['max_seq_length'] = seq_lengths.max()\n",
    "            \n",
    "            summaries.append(summary)\n",
    "    \n",
    "    return pd.DataFrame(summaries)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Utility functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Metadata Analysis\n",
    "\n",
    "### 3.1 Overview of All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD ALL TRAINING METADATA\n",
    "# =============================================================================\n",
    "\n",
    "# Load metadata from all training datasets\n",
    "train_metadata = load_all_metadata(TRAIN_DIR, dataset_prefix=\"train_dataset_\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING METADATA OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìä Total repertoires (samples): {len(train_metadata):,}\")\n",
    "print(f\"üìÅ Number of datasets: {train_metadata['dataset'].nunique()}\")\n",
    "print(f\"\\nüìã Available columns in metadata:\")\n",
    "for col in train_metadata.columns:\n",
    "    non_null = train_metadata[col].notna().sum()\n",
    "    unique = train_metadata[col].nunique()\n",
    "    print(f\"   ‚Ä¢ {col}: {non_null:,} non-null ({non_null/len(train_metadata)*100:.1f}%), {unique:,} unique values\")\n",
    "\n",
    "# Display sample of the data\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE DATA (first 10 rows)\")\n",
    "print(\"=\" * 80)\n",
    "display(train_metadata.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Class Balance Analysis (Label Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLASS BALANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# --- Overall class distribution ---\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "if 'label_positive' in train_metadata.columns:\n",
    "    class_counts = train_metadata['label_positive'].value_counts()\n",
    "    colors = ['#2ecc71', '#e74c3c']  # Green for True, Red for False\n",
    "    ax1.pie(class_counts, labels=['Positive (Disease)', 'Negative (Healthy)'], \n",
    "            autopct='%1.1f%%', colors=colors, explode=(0.02, 0.02),\n",
    "            shadow=True, startangle=90)\n",
    "    ax1.set_title('Overall Class Distribution', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"OVERALL CLASS BALANCE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nüìà Positive (Disease): {class_counts.get(True, 0):,} ({class_counts.get(True, 0)/len(train_metadata)*100:.1f}%)\")\n",
    "    print(f\"üìâ Negative (Healthy): {class_counts.get(False, 0):,} ({class_counts.get(False, 0)/len(train_metadata)*100:.1f}%)\")\n",
    "    print(f\"‚öñÔ∏è  Imbalance Ratio: {max(class_counts)/min(class_counts):.2f}:1\")\n",
    "\n",
    "# --- Class distribution per dataset ---\n",
    "ax2 = fig.add_subplot(gs[0, 1:])\n",
    "if 'label_positive' in train_metadata.columns:\n",
    "    dataset_class_counts = train_metadata.groupby(['dataset', 'label_positive']).size().unstack(fill_value=0)\n",
    "    dataset_class_counts.plot(kind='bar', stacked=True, ax=ax2, color=['#e74c3c', '#2ecc71'])\n",
    "    ax2.set_xlabel('Dataset', fontsize=10)\n",
    "    ax2.set_ylabel('Number of Repertoires', fontsize=10)\n",
    "    ax2.set_title('Class Distribution by Dataset', fontsize=12, fontweight='bold')\n",
    "    ax2.legend(['Negative', 'Positive'], loc='upper right')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# --- Repertoires per dataset ---\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "dataset_sizes = train_metadata['dataset'].value_counts().sort_index()\n",
    "ax3.bar(range(len(dataset_sizes)), dataset_sizes.values, color='steelblue', alpha=0.7)\n",
    "ax3.set_xlabel('Dataset Index', fontsize=10)\n",
    "ax3.set_ylabel('Number of Repertoires', fontsize=10)\n",
    "ax3.set_title('Repertoires per Dataset', fontsize=12, fontweight='bold')\n",
    "ax3.axhline(y=dataset_sizes.mean(), color='red', linestyle='--', label=f'Mean: {dataset_sizes.mean():.0f}')\n",
    "ax3.legend()\n",
    "\n",
    "# --- Summary statistics table ---\n",
    "ax4 = fig.add_subplot(gs[1, 1:])\n",
    "ax4.axis('off')\n",
    "\n",
    "if 'label_positive' in train_metadata.columns:\n",
    "    summary_data = []\n",
    "    for dataset in sorted(train_metadata['dataset'].unique()):\n",
    "        ds_data = train_metadata[train_metadata['dataset'] == dataset]\n",
    "        n_pos = ds_data['label_positive'].sum()\n",
    "        n_neg = len(ds_data) - n_pos\n",
    "        ratio = n_pos / n_neg if n_neg > 0 else float('inf')\n",
    "        summary_data.append({\n",
    "            'Dataset': dataset.replace('train_dataset_', 'DS_'),\n",
    "            'Total': len(ds_data),\n",
    "            'Positive': n_pos,\n",
    "            'Negative': n_neg,\n",
    "            'Pos %': f\"{n_pos/len(ds_data)*100:.1f}%\",\n",
    "            'Ratio': f\"{ratio:.2f}\"\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    table = ax4.table(cellText=summary_df.values,\n",
    "                      colLabels=summary_df.columns,\n",
    "                      loc='center',\n",
    "                      cellLoc='center',\n",
    "                      colColours=['#4a90d9']*len(summary_df.columns))\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 1.5)\n",
    "    ax4.set_title('Class Balance Summary by Dataset', fontsize=12, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed printout\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED CLASS BALANCE BY DATASET\")\n",
    "print(\"=\" * 80)\n",
    "if 'label_positive' in train_metadata.columns:\n",
    "    display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Demographic Features Analysis (Age, Sex, Race, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DEMOGRAPHIC FEATURES ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# Identify demographic columns\n",
    "demographic_cols = ['age', 'sex', 'race', 'study_group', 'subject_id']\n",
    "available_demo_cols = [col for col in demographic_cols if col in train_metadata.columns]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DEMOGRAPHIC FEATURES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìã Available demographic columns: {available_demo_cols}\")\n",
    "\n",
    "if available_demo_cols:\n",
    "    # Create visualizations for each demographic feature\n",
    "    n_cols = min(3, len(available_demo_cols))\n",
    "    n_rows = (len(available_demo_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    elif n_cols == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for idx, col in enumerate(available_demo_cols):\n",
    "        row_idx, col_idx = idx // n_cols, idx % n_cols\n",
    "        ax = axes[row_idx, col_idx]\n",
    "        \n",
    "        if train_metadata[col].dtype in ['float64', 'int64'] and train_metadata[col].nunique() > 10:\n",
    "            # Continuous variable - histogram\n",
    "            if 'label_positive' in train_metadata.columns:\n",
    "                for label, color in [(True, '#2ecc71'), (False, '#e74c3c')]:\n",
    "                    subset = train_metadata[train_metadata['label_positive'] == label][col].dropna()\n",
    "                    ax.hist(subset, bins=30, alpha=0.6, label=f'{\"Positive\" if label else \"Negative\"}', color=color)\n",
    "                ax.legend()\n",
    "            else:\n",
    "                ax.hist(train_metadata[col].dropna(), bins=30, alpha=0.7, color='steelblue')\n",
    "            ax.set_xlabel(col, fontsize=10)\n",
    "            ax.set_ylabel('Count', fontsize=10)\n",
    "        else:\n",
    "            # Categorical variable - bar chart\n",
    "            if 'label_positive' in train_metadata.columns:\n",
    "                cross_tab = pd.crosstab(train_metadata[col], train_metadata['label_positive'])\n",
    "                cross_tab.plot(kind='bar', ax=ax, color=['#e74c3c', '#2ecc71'], alpha=0.8)\n",
    "                ax.legend(['Negative', 'Positive'])\n",
    "            else:\n",
    "                train_metadata[col].value_counts().head(15).plot(kind='bar', ax=ax, color='steelblue', alpha=0.8)\n",
    "            ax.set_xlabel(col, fontsize=10)\n",
    "            ax.set_ylabel('Count', fontsize=10)\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        ax.set_title(f'{col.upper()} Distribution', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(available_demo_cols), n_rows * n_cols):\n",
    "        row_idx, col_idx = idx // n_cols, idx % n_cols\n",
    "        axes[row_idx, col_idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    for col in available_demo_cols:\n",
    "        print(f\"\\nüìä {col.upper()} Distribution:\")\n",
    "        if train_metadata[col].dtype in ['float64', 'int64'] and train_metadata[col].nunique() > 10:\n",
    "            print(train_metadata[col].describe())\n",
    "        else:\n",
    "            print(train_metadata[col].value_counts().head(10))\n",
    "        print(f\"   Missing: {train_metadata[col].isna().sum()} ({train_metadata[col].isna().sum()/len(train_metadata)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No demographic columns found in metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Feature Correlations with Target Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CORRELATION ANALYSIS WITH TARGET LABEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CORRELATION ANALYSIS WITH TARGET LABEL (label_positive)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'label_positive' in train_metadata.columns:\n",
    "    # Convert label to numeric for correlation\n",
    "    train_metadata['label_numeric'] = train_metadata['label_positive'].astype(int)\n",
    "    \n",
    "    correlation_results = []\n",
    "    \n",
    "    # Analyze each column\n",
    "    for col in train_metadata.columns:\n",
    "        if col in ['label_positive', 'label_numeric', 'repertoire_id', 'filename', 'dataset']:\n",
    "            continue\n",
    "        \n",
    "        col_data = train_metadata[col].dropna()\n",
    "        if len(col_data) < 10:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            if train_metadata[col].dtype in ['float64', 'int64']:\n",
    "                # Continuous: Pearson correlation + t-test\n",
    "                valid_mask = train_metadata[col].notna()\n",
    "                correlation = train_metadata.loc[valid_mask, col].corr(train_metadata.loc[valid_mask, 'label_numeric'])\n",
    "                \n",
    "                # T-test between groups\n",
    "                pos_vals = train_metadata[train_metadata['label_positive'] == True][col].dropna()\n",
    "                neg_vals = train_metadata[train_metadata['label_positive'] == False][col].dropna()\n",
    "                if len(pos_vals) > 1 and len(neg_vals) > 1:\n",
    "                    t_stat, p_val = stats.ttest_ind(pos_vals, neg_vals)\n",
    "                else:\n",
    "                    t_stat, p_val = np.nan, np.nan\n",
    "                \n",
    "                correlation_results.append({\n",
    "                    'Feature': col,\n",
    "                    'Type': 'Continuous',\n",
    "                    'Correlation': correlation,\n",
    "                    'Test': 't-test',\n",
    "                    'Statistic': t_stat,\n",
    "                    'P-value': p_val,\n",
    "                    'Significant': p_val < 0.05 if not np.isnan(p_val) else False\n",
    "                })\n",
    "            else:\n",
    "                # Categorical: Chi-squared test\n",
    "                contingency = pd.crosstab(train_metadata[col], train_metadata['label_positive'])\n",
    "                if contingency.shape[0] > 1 and contingency.shape[1] > 1:\n",
    "                    chi2, p_val, dof, expected = stats.chi2_contingency(contingency)\n",
    "                    \n",
    "                    # Cram√©r's V for effect size\n",
    "                    n = contingency.sum().sum()\n",
    "                    min_dim = min(contingency.shape) - 1\n",
    "                    cramers_v = np.sqrt(chi2 / (n * min_dim)) if min_dim > 0 else 0\n",
    "                    \n",
    "                    correlation_results.append({\n",
    "                        'Feature': col,\n",
    "                        'Type': 'Categorical',\n",
    "                        'Correlation': cramers_v,\n",
    "                        'Test': 'Chi-squared',\n",
    "                        'Statistic': chi2,\n",
    "                        'P-value': p_val,\n",
    "                        'Significant': p_val < 0.05\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"   Could not analyze {col}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if correlation_results:\n",
    "        corr_df = pd.DataFrame(correlation_results).sort_values('P-value')\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Correlation/Effect size bar chart\n",
    "        ax1 = axes[0]\n",
    "        colors = ['#2ecc71' if sig else '#95a5a1' for sig in corr_df['Significant']]\n",
    "        bars = ax1.barh(corr_df['Feature'], corr_df['Correlation'].abs(), color=colors, alpha=0.8)\n",
    "        ax1.set_xlabel('Correlation / Effect Size (Cram√©r\\'s V)', fontsize=10)\n",
    "        ax1.set_title('Feature Association with Label', fontsize=12, fontweight='bold')\n",
    "        ax1.axvline(x=0.1, color='red', linestyle='--', alpha=0.5, label='Small effect')\n",
    "        ax1.axvline(x=0.3, color='orange', linestyle='--', alpha=0.5, label='Medium effect')\n",
    "        ax1.legend(fontsize=8)\n",
    "        \n",
    "        # P-value bar chart (log scale)\n",
    "        ax2 = axes[1]\n",
    "        log_pvals = -np.log10(corr_df['P-value'].replace(0, 1e-300))\n",
    "        colors = ['#e74c3c' if sig else '#95a5a1' for sig in corr_df['Significant']]\n",
    "        ax2.barh(corr_df['Feature'], log_pvals, color=colors, alpha=0.8)\n",
    "        ax2.axvline(x=-np.log10(0.05), color='red', linestyle='--', label='p=0.05')\n",
    "        ax2.axvline(x=-np.log10(0.01), color='orange', linestyle='--', label='p=0.01')\n",
    "        ax2.set_xlabel('-log10(P-value)', fontsize=10)\n",
    "        ax2.set_title('Statistical Significance', fontsize=12, fontweight='bold')\n",
    "        ax2.legend(fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display results table\n",
    "        print(\"\\nüìä Correlation Analysis Results (sorted by significance):\")\n",
    "        display(corr_df.style.format({\n",
    "            'Correlation': '{:.4f}',\n",
    "            'Statistic': '{:.2f}',\n",
    "            'P-value': '{:.2e}'\n",
    "        }).applymap(lambda x: 'background-color: #d4edda' if x == True else '', subset=['Significant']))\n",
    "    \n",
    "    # Clean up\n",
    "    train_metadata.drop('label_numeric', axis=1, inplace=True)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è label_positive column not found in metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sequence-Level Analysis\n",
    "\n",
    "### 4.1 Repertoire Structure Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# REPERTOIRE STRUCTURE OVERVIEW\n",
    "# =============================================================================\n",
    "# Analyze repertoire files from a sample of datasets to understand structure\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"REPERTOIRE STRUCTURE OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get list of training datasets\n",
    "train_datasets = sorted([d for d in os.listdir(TRAIN_DIR) if d.startswith(\"train_dataset_\")])\n",
    "\n",
    "# Analyze structure from first dataset\n",
    "sample_dataset = train_datasets[0]\n",
    "sample_dataset_path = os.path.join(TRAIN_DIR, sample_dataset)\n",
    "\n",
    "# Load a sample repertoire to examine structure\n",
    "sample_files = glob.glob(os.path.join(sample_dataset_path, \"*.tsv\"))[:1]\n",
    "if sample_files:\n",
    "    sample_rep = pd.read_csv(sample_files[0], sep='\\t')\n",
    "    \n",
    "    print(f\"\\nüìÅ Sample dataset: {sample_dataset}\")\n",
    "    print(f\"üìÑ Sample file: {os.path.basename(sample_files[0])}\")\n",
    "    print(f\"\\nüìã Columns in repertoire files:\")\n",
    "    for col in sample_rep.columns:\n",
    "        dtype = sample_rep[col].dtype\n",
    "        n_unique = sample_rep[col].nunique()\n",
    "        n_null = sample_rep[col].isna().sum()\n",
    "        print(f\"   ‚Ä¢ {col}: {dtype} | {n_unique:,} unique | {n_null} null\")\n",
    "    \n",
    "    print(f\"\\nüìä Sample repertoire shape: {sample_rep.shape}\")\n",
    "    print(f\"   ‚Ä¢ Rows (sequences): {len(sample_rep):,}\")\n",
    "    print(f\"   ‚Ä¢ Columns: {len(sample_rep.columns)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAMPLE REPERTOIRE DATA (first 10 rows)\")\n",
    "    print(\"=\" * 80)\n",
    "    display(sample_rep.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Repertoire Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# REPERTOIRE SIZE DISTRIBUTION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def get_repertoire_sizes(base_dir: str, datasets: list, sample_per_dataset: int = None) -> pd.DataFrame:\n",
    "    \"\"\"Get the number of sequences per repertoire across datasets.\"\"\"\n",
    "    sizes = []\n",
    "    \n",
    "    for dataset_name in tqdm(datasets, desc=\"Analyzing repertoire sizes\"):\n",
    "        dataset_path = os.path.join(base_dir, dataset_name)\n",
    "        metadata_path = os.path.join(dataset_path, 'metadata.csv')\n",
    "        \n",
    "        if os.path.exists(metadata_path):\n",
    "            metadata = pd.read_csv(metadata_path)\n",
    "            files_to_check = metadata['filename'].tolist()\n",
    "            labels = dict(zip(metadata['filename'], metadata['label_positive']))\n",
    "        else:\n",
    "            files_to_check = [os.path.basename(f) for f in glob.glob(os.path.join(dataset_path, \"*.tsv\"))]\n",
    "            labels = {}\n",
    "        \n",
    "        if sample_per_dataset:\n",
    "            files_to_check = files_to_check[:sample_per_dataset]\n",
    "        \n",
    "        for filename in files_to_check:\n",
    "            file_path = os.path.join(dataset_path, filename)\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    # Just count lines without loading full file for efficiency\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        n_lines = sum(1 for _ in f) - 1  # Subtract header\n",
    "                    \n",
    "                    sizes.append({\n",
    "                        'dataset': dataset_name,\n",
    "                        'filename': filename,\n",
    "                        'n_sequences': n_lines,\n",
    "                        'label_positive': labels.get(filename, None)\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "    \n",
    "    return pd.DataFrame(sizes)\n",
    "\n",
    "# Get repertoire sizes (sample for speed)\n",
    "print(\"=\" * 80)\n",
    "print(\"REPERTOIRE SIZE DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "repertoire_sizes = get_repertoire_sizes(TRAIN_DIR, train_datasets, sample_per_dataset=50)\n",
    "\n",
    "if not repertoire_sizes.empty:\n",
    "    print(f\"\\nüìä Analyzed {len(repertoire_sizes):,} repertoires across {repertoire_sizes['dataset'].nunique()} datasets\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Overall distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.hist(repertoire_sizes['n_sequences'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    ax1.axvline(repertoire_sizes['n_sequences'].median(), color='red', linestyle='--', \n",
    "                label=f'Median: {repertoire_sizes[\"n_sequences\"].median():,.0f}')\n",
    "    ax1.axvline(repertoire_sizes['n_sequences'].mean(), color='orange', linestyle='--', \n",
    "                label=f'Mean: {repertoire_sizes[\"n_sequences\"].mean():,.0f}')\n",
    "    ax1.set_xlabel('Number of Sequences', fontsize=10)\n",
    "    ax1.set_ylabel('Count', fontsize=10)\n",
    "    ax1.set_title('Repertoire Size Distribution (All Datasets)', fontsize=12, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Log scale distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(np.log10(repertoire_sizes['n_sequences'] + 1), bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    ax2.set_xlabel('log10(Number of Sequences)', fontsize=10)\n",
    "    ax2.set_ylabel('Count', fontsize=10)\n",
    "    ax2.set_title('Repertoire Size Distribution (Log Scale)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Distribution by label\n",
    "    ax3 = axes[1, 0]\n",
    "    if repertoire_sizes['label_positive'].notna().any():\n",
    "        for label, color in [(True, '#2ecc71'), (False, '#e74c3c')]:\n",
    "            subset = repertoire_sizes[repertoire_sizes['label_positive'] == label]['n_sequences']\n",
    "            if len(subset) > 0:\n",
    "                ax3.hist(subset, bins=30, alpha=0.6, label=f'{\"Positive\" if label else \"Negative\"}', color=color)\n",
    "        ax3.legend()\n",
    "    ax3.set_xlabel('Number of Sequences', fontsize=10)\n",
    "    ax3.set_ylabel('Count', fontsize=10)\n",
    "    ax3.set_title('Repertoire Size by Label', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Box plot by dataset\n",
    "    ax4 = axes[1, 1]\n",
    "    dataset_order = repertoire_sizes.groupby('dataset')['n_sequences'].median().sort_values().index\n",
    "    sns.boxplot(data=repertoire_sizes, x='dataset', y='n_sequences', ax=ax4, order=dataset_order)\n",
    "    ax4.set_xlabel('Dataset', fontsize=10)\n",
    "    ax4.set_ylabel('Number of Sequences', fontsize=10)\n",
    "    ax4.set_title('Repertoire Size by Dataset', fontsize=12, fontweight='bold')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nüìà Repertoire Size Statistics:\")\n",
    "    print(repertoire_sizes['n_sequences'].describe())\n",
    "    \n",
    "    if repertoire_sizes['label_positive'].notna().any():\n",
    "        print(\"\\nüìä Size Statistics by Label:\")\n",
    "        display(repertoire_sizes.groupby('label_positive')['n_sequences'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Junction AA Sequence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# JUNCTION_AA SEQUENCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_sequences_from_dataset(dataset_path: str, n_files: int = 10) -> Dict:\n",
    "    \"\"\"Analyze junction_aa sequences from a sample of repertoire files.\"\"\"\n",
    "    metadata_path = os.path.join(dataset_path, 'metadata.csv')\n",
    "    \n",
    "    if os.path.exists(metadata_path):\n",
    "        metadata = pd.read_csv(metadata_path)\n",
    "        files = metadata['filename'].tolist()[:n_files]\n",
    "        labels = dict(zip(metadata['filename'], metadata['label_positive']))\n",
    "    else:\n",
    "        files = [os.path.basename(f) for f in glob.glob(os.path.join(dataset_path, \"*.tsv\"))[:n_files]]\n",
    "        labels = {}\n",
    "    \n",
    "    all_seq_lengths = []\n",
    "    all_sequences = []\n",
    "    amino_acid_counts = Counter()\n",
    "    \n",
    "    for filename in files:\n",
    "        file_path = os.path.join(dataset_path, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            rep = pd.read_csv(file_path, sep='\\t')\n",
    "            if 'junction_aa' in rep.columns:\n",
    "                seqs = rep['junction_aa'].dropna()\n",
    "                all_sequences.extend(seqs.tolist())\n",
    "                \n",
    "                lengths = seqs.str.len()\n",
    "                label = labels.get(filename, None)\n",
    "                for length in lengths:\n",
    "                    all_seq_lengths.append({'length': length, 'label': label})\n",
    "                \n",
    "                # Count amino acids\n",
    "                for seq in seqs:\n",
    "                    amino_acid_counts.update(seq)\n",
    "    \n",
    "    return {\n",
    "        'seq_lengths': pd.DataFrame(all_seq_lengths),\n",
    "        'sequences': all_sequences,\n",
    "        'amino_acid_counts': amino_acid_counts\n",
    "    }\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"JUNCTION_AA SEQUENCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze sequences from first few datasets\n",
    "sequence_analysis = {}\n",
    "for dataset_name in train_datasets[:3]:\n",
    "    dataset_path = os.path.join(TRAIN_DIR, dataset_name)\n",
    "    sequence_analysis[dataset_name] = analyze_sequences_from_dataset(dataset_path, n_files=20)\n",
    "    print(f\"‚úÖ Analyzed {dataset_name}\")\n",
    "\n",
    "# Combine results\n",
    "all_lengths = pd.concat([sa['seq_lengths'] for sa in sequence_analysis.values()], ignore_index=True)\n",
    "all_aa_counts = Counter()\n",
    "for sa in sequence_analysis.values():\n",
    "    all_aa_counts.update(sa['amino_acid_counts'])\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Sequence length distribution\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(all_lengths['length'].dropna(), bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(all_lengths['length'].median(), color='red', linestyle='--', \n",
    "            label=f'Median: {all_lengths[\"length\"].median():.0f}')\n",
    "ax1.set_xlabel('Junction AA Length', fontsize=10)\n",
    "ax1.set_ylabel('Count', fontsize=10)\n",
    "ax1.set_title('Junction AA Sequence Length Distribution', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "\n",
    "# Length by label\n",
    "ax2 = axes[0, 1]\n",
    "if all_lengths['label'].notna().any():\n",
    "    for label, color in [(True, '#2ecc71'), (False, '#e74c3c')]:\n",
    "        subset = all_lengths[all_lengths['label'] == label]['length']\n",
    "        if len(subset) > 0:\n",
    "            ax2.hist(subset, bins=30, alpha=0.6, label=f'{\"Positive\" if label else \"Negative\"}', color=color)\n",
    "    ax2.legend()\n",
    "ax2.set_xlabel('Junction AA Length', fontsize=10)\n",
    "ax2.set_ylabel('Count', fontsize=10)\n",
    "ax2.set_title('Sequence Length by Label', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Amino acid frequency\n",
    "ax3 = axes[1, 0]\n",
    "aa_df = pd.DataFrame.from_dict(all_aa_counts, orient='index', columns=['count']).sort_values('count', ascending=True)\n",
    "# Filter to standard amino acids\n",
    "standard_aa = list('ACDEFGHIKLMNPQRSTVWY')\n",
    "aa_df_filtered = aa_df[aa_df.index.isin(standard_aa)].sort_values('count', ascending=True)\n",
    "ax3.barh(aa_df_filtered.index, aa_df_filtered['count'], color='steelblue', alpha=0.8)\n",
    "ax3.set_xlabel('Count', fontsize=10)\n",
    "ax3.set_ylabel('Amino Acid', fontsize=10)\n",
    "ax3.set_title('Amino Acid Frequency in Junction AA', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Length statistics by dataset\n",
    "ax4 = axes[1, 1]\n",
    "length_stats = []\n",
    "for dataset_name, sa in sequence_analysis.items():\n",
    "    lengths = sa['seq_lengths']['length']\n",
    "    length_stats.append({\n",
    "        'Dataset': dataset_name.replace('train_dataset_', 'DS_'),\n",
    "        'Mean': lengths.mean(),\n",
    "        'Median': lengths.median(),\n",
    "        'Std': lengths.std()\n",
    "    })\n",
    "length_df = pd.DataFrame(length_stats)\n",
    "x = np.arange(len(length_df))\n",
    "width = 0.35\n",
    "ax4.bar(x - width/2, length_df['Mean'], width, label='Mean', color='steelblue', alpha=0.8)\n",
    "ax4.bar(x + width/2, length_df['Median'], width, label='Median', color='coral', alpha=0.8)\n",
    "ax4.errorbar(x - width/2, length_df['Mean'], yerr=length_df['Std'], fmt='none', color='black', capsize=3)\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(length_df['Dataset'], rotation=45)\n",
    "ax4.set_ylabel('Length', fontsize=10)\n",
    "ax4.set_title('Sequence Length Statistics by Dataset', fontsize=12, fontweight='bold')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nüìä Junction AA Length Statistics:\")\n",
    "print(all_lengths['length'].describe())\n",
    "\n",
    "print(\"\\nüìä Top 10 Most Common Amino Acids:\")\n",
    "print(aa_df_filtered.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 V/J/D Gene Call Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# V/J/D GENE CALL DISTRIBUTION\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_gene_calls(dataset_path: str, n_files: int = 20) -> Dict:\n",
    "    \"\"\"Analyze v_call, j_call, and d_call distributions.\"\"\"\n",
    "    metadata_path = os.path.join(dataset_path, 'metadata.csv')\n",
    "    \n",
    "    if os.path.exists(metadata_path):\n",
    "        metadata = pd.read_csv(metadata_path)\n",
    "        files = metadata['filename'].tolist()[:n_files]\n",
    "        labels = dict(zip(metadata['filename'], metadata['label_positive']))\n",
    "    else:\n",
    "        files = [os.path.basename(f) for f in glob.glob(os.path.join(dataset_path, \"*.tsv\"))[:n_files]]\n",
    "        labels = {}\n",
    "    \n",
    "    v_calls = Counter()\n",
    "    j_calls = Counter()\n",
    "    d_calls = Counter()\n",
    "    \n",
    "    v_calls_by_label = {True: Counter(), False: Counter()}\n",
    "    j_calls_by_label = {True: Counter(), False: Counter()}\n",
    "    \n",
    "    has_d_call = False\n",
    "    \n",
    "    for filename in files:\n",
    "        file_path = os.path.join(dataset_path, filename)\n",
    "        label = labels.get(filename, None)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            rep = pd.read_csv(file_path, sep='\\t')\n",
    "            \n",
    "            if 'v_call' in rep.columns:\n",
    "                v_vals = rep['v_call'].dropna()\n",
    "                v_calls.update(v_vals)\n",
    "                if label is not None:\n",
    "                    v_calls_by_label[label].update(v_vals)\n",
    "            \n",
    "            if 'j_call' in rep.columns:\n",
    "                j_vals = rep['j_call'].dropna()\n",
    "                j_calls.update(j_vals)\n",
    "                if label is not None:\n",
    "                    j_calls_by_label[label].update(j_vals)\n",
    "            \n",
    "            if 'd_call' in rep.columns:\n",
    "                has_d_call = True\n",
    "                d_calls.update(rep['d_call'].dropna())\n",
    "    \n",
    "    return {\n",
    "        'v_calls': v_calls,\n",
    "        'j_calls': j_calls,\n",
    "        'd_calls': d_calls,\n",
    "        'has_d_call': has_d_call,\n",
    "        'v_calls_by_label': v_calls_by_label,\n",
    "        'j_calls_by_label': j_calls_by_label\n",
    "    }\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"V/J/D GENE CALL DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze gene calls from datasets\n",
    "gene_analysis = {}\n",
    "for dataset_name in train_datasets[:3]:\n",
    "    dataset_path = os.path.join(TRAIN_DIR, dataset_name)\n",
    "    gene_analysis[dataset_name] = analyze_gene_calls(dataset_path, n_files=30)\n",
    "    print(f\"‚úÖ Analyzed {dataset_name}: d_call present = {gene_analysis[dataset_name]['has_d_call']}\")\n",
    "\n",
    "# Combine results\n",
    "combined_v = Counter()\n",
    "combined_j = Counter()\n",
    "combined_d = Counter()\n",
    "for ga in gene_analysis.values():\n",
    "    combined_v.update(ga['v_calls'])\n",
    "    combined_j.update(ga['j_calls'])\n",
    "    combined_d.update(ga['d_calls'])\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Top V genes\n",
    "ax1 = axes[0, 0]\n",
    "top_v = pd.DataFrame.from_dict(combined_v, orient='index', columns=['count']).nlargest(20, 'count')\n",
    "ax1.barh(top_v.index, top_v['count'], color='steelblue', alpha=0.8)\n",
    "ax1.set_xlabel('Count', fontsize=10)\n",
    "ax1.set_ylabel('V Gene', fontsize=10)\n",
    "ax1.set_title('Top 20 V Gene Calls', fontsize=12, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Top J genes\n",
    "ax2 = axes[0, 1]\n",
    "top_j = pd.DataFrame.from_dict(combined_j, orient='index', columns=['count']).nlargest(20, 'count')\n",
    "ax2.barh(top_j.index, top_j['count'], color='coral', alpha=0.8)\n",
    "ax2.set_xlabel('Count', fontsize=10)\n",
    "ax2.set_ylabel('J Gene', fontsize=10)\n",
    "ax2.set_title('Top 20 J Gene Calls', fontsize=12, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# V gene diversity by dataset\n",
    "ax3 = axes[1, 0]\n",
    "v_diversity = []\n",
    "for dataset_name, ga in gene_analysis.items():\n",
    "    n_unique_v = len(ga['v_calls'])\n",
    "    total_v = sum(ga['v_calls'].values())\n",
    "    v_diversity.append({\n",
    "        'Dataset': dataset_name.replace('train_dataset_', 'DS_'),\n",
    "        'Unique V genes': n_unique_v,\n",
    "        'Total V calls': total_v\n",
    "    })\n",
    "v_div_df = pd.DataFrame(v_diversity)\n",
    "x = np.arange(len(v_div_df))\n",
    "ax3.bar(x, v_div_df['Unique V genes'], color='steelblue', alpha=0.8)\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(v_div_df['Dataset'], rotation=45)\n",
    "ax3.set_ylabel('Number of Unique V Genes', fontsize=10)\n",
    "ax3.set_title('V Gene Diversity by Dataset', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Top D genes (if available)\n",
    "ax4 = axes[1, 1]\n",
    "if combined_d:\n",
    "    top_d = pd.DataFrame.from_dict(combined_d, orient='index', columns=['count']).nlargest(15, 'count')\n",
    "    ax4.barh(top_d.index, top_d['count'], color='forestgreen', alpha=0.8)\n",
    "    ax4.set_xlabel('Count', fontsize=10)\n",
    "    ax4.set_ylabel('D Gene', fontsize=10)\n",
    "    ax4.set_title('Top 15 D Gene Calls', fontsize=12, fontweight='bold')\n",
    "    ax4.invert_yaxis()\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'D gene calls not available\\nin analyzed datasets', \n",
    "             ha='center', va='center', fontsize=12, transform=ax4.transAxes)\n",
    "    ax4.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nüìä Gene Call Statistics:\")\n",
    "print(f\"   ‚Ä¢ Unique V genes: {len(combined_v):,}\")\n",
    "print(f\"   ‚Ä¢ Unique J genes: {len(combined_j):,}\")\n",
    "print(f\"   ‚Ä¢ Unique D genes: {len(combined_d):,}\")\n",
    "\n",
    "print(\"\\nüìä Top 10 V Genes:\")\n",
    "for gene, count in combined_v.most_common(10):\n",
    "    print(f\"   {gene}: {count:,}\")\n",
    "\n",
    "print(\"\\nüìä Top 10 J Genes:\")\n",
    "for gene, count in combined_j.most_common(10):\n",
    "    print(f\"   {gene}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Diversity Metrics & Sequence Sharing\n",
    "\n",
    "### 5.1 Repertoire Diversity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# REPERTOIRE DIVERSITY METRICS\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_diversity_metrics(dataset_path: str, n_files: int = 30) -> pd.DataFrame:\n",
    "    \"\"\"Calculate diversity metrics for repertoires in a dataset.\"\"\"\n",
    "    metadata_path = os.path.join(dataset_path, 'metadata.csv')\n",
    "    \n",
    "    if os.path.exists(metadata_path):\n",
    "        metadata = pd.read_csv(metadata_path)\n",
    "        files = metadata['filename'].tolist()[:n_files]\n",
    "        labels = dict(zip(metadata['filename'], metadata['label_positive']))\n",
    "    else:\n",
    "        files = [os.path.basename(f) for f in glob.glob(os.path.join(dataset_path, \"*.tsv\"))[:n_files]]\n",
    "        labels = {}\n",
    "    \n",
    "    metrics = []\n",
    "    \n",
    "    for filename in tqdm(files, desc=f\"Calculating diversity\", leave=False):\n",
    "        file_path = os.path.join(dataset_path, filename)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            rep = pd.read_csv(file_path, sep='\\t')\n",
    "            \n",
    "            metric = {\n",
    "                'filename': filename,\n",
    "                'label_positive': labels.get(filename, None),\n",
    "                'n_sequences': len(rep),\n",
    "            }\n",
    "            \n",
    "            # Junction AA diversity\n",
    "            if 'junction_aa' in rep.columns:\n",
    "                junction_counts = rep['junction_aa'].value_counts()\n",
    "                n_unique = len(junction_counts)\n",
    "                metric['n_unique_junction'] = n_unique\n",
    "                metric['junction_richness'] = n_unique / len(rep) if len(rep) > 0 else 0\n",
    "                \n",
    "                # Shannon entropy\n",
    "                probs = junction_counts / junction_counts.sum()\n",
    "                metric['junction_entropy'] = entropy(probs)\n",
    "                \n",
    "                # Simpson's diversity index\n",
    "                metric['simpson_diversity'] = 1 - sum((junction_counts / junction_counts.sum()) ** 2)\n",
    "                \n",
    "                # Clonality (inverse of normalized entropy)\n",
    "                max_entropy = np.log(n_unique) if n_unique > 1 else 1\n",
    "                metric['clonality'] = 1 - (metric['junction_entropy'] / max_entropy) if max_entropy > 0 else 0\n",
    "            \n",
    "            # V gene diversity\n",
    "            if 'v_call' in rep.columns:\n",
    "                v_counts = rep['v_call'].value_counts()\n",
    "                metric['n_unique_v'] = len(v_counts)\n",
    "                metric['v_entropy'] = entropy(v_counts / v_counts.sum())\n",
    "            \n",
    "            # J gene diversity  \n",
    "            if 'j_call' in rep.columns:\n",
    "                j_counts = rep['j_call'].value_counts()\n",
    "                metric['n_unique_j'] = len(j_counts)\n",
    "                metric['j_entropy'] = entropy(j_counts / j_counts.sum())\n",
    "            \n",
    "            # Templates/duplicate counts if available\n",
    "            template_col = 'templates' if 'templates' in rep.columns else ('duplicate_count' if 'duplicate_count' in rep.columns else None)\n",
    "            if template_col:\n",
    "                metric['has_templates'] = True\n",
    "                metric['total_templates'] = rep[template_col].sum()\n",
    "                metric['mean_templates'] = rep[template_col].mean()\n",
    "                metric['max_templates'] = rep[template_col].max()\n",
    "            else:\n",
    "                metric['has_templates'] = False\n",
    "            \n",
    "            metrics.append(metric)\n",
    "    \n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"REPERTOIRE DIVERSITY METRICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate diversity for multiple datasets\n",
    "diversity_results = []\n",
    "for dataset_name in train_datasets[:5]:\n",
    "    dataset_path = os.path.join(TRAIN_DIR, dataset_name)\n",
    "    div_df = calculate_diversity_metrics(dataset_path, n_files=30)\n",
    "    div_df['dataset'] = dataset_name\n",
    "    diversity_results.append(div_df)\n",
    "    print(f\"‚úÖ Calculated diversity for {dataset_name}\")\n",
    "\n",
    "diversity_df = pd.concat(diversity_results, ignore_index=True)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Shannon entropy distribution\n",
    "ax1 = axes[0, 0]\n",
    "if 'junction_entropy' in diversity_df.columns:\n",
    "    if diversity_df['label_positive'].notna().any():\n",
    "        for label, color in [(True, '#2ecc71'), (False, '#e74c3c')]:\n",
    "            subset = diversity_df[diversity_df['label_positive'] == label]['junction_entropy'].dropna()\n",
    "            if len(subset) > 0:\n",
    "                ax1.hist(subset, bins=20, alpha=0.6, label=f'{\"Positive\" if label else \"Negative\"}', color=color)\n",
    "        ax1.legend()\n",
    "    else:\n",
    "        ax1.hist(diversity_df['junction_entropy'].dropna(), bins=20, color='steelblue', alpha=0.7)\n",
    "ax1.set_xlabel('Shannon Entropy', fontsize=10)\n",
    "ax1.set_ylabel('Count', fontsize=10)\n",
    "ax1.set_title('Junction AA Entropy Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Richness by label\n",
    "ax2 = axes[0, 1]\n",
    "if 'junction_richness' in diversity_df.columns:\n",
    "    if diversity_df['label_positive'].notna().any():\n",
    "        sns.boxplot(data=diversity_df, x='label_positive', y='junction_richness', ax=ax2, \n",
    "                   palette={True: '#2ecc71', False: '#e74c3c'})\n",
    "    else:\n",
    "        ax2.hist(diversity_df['junction_richness'].dropna(), bins=20, color='steelblue', alpha=0.7)\n",
    "ax2.set_xlabel('Label', fontsize=10)\n",
    "ax2.set_ylabel('Richness (Unique/Total)', fontsize=10)\n",
    "ax2.set_title('Junction Richness by Label', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Clonality distribution\n",
    "ax3 = axes[0, 2]\n",
    "if 'clonality' in diversity_df.columns:\n",
    "    if diversity_df['label_positive'].notna().any():\n",
    "        for label, color in [(True, '#2ecc71'), (False, '#e74c3c')]:\n",
    "            subset = diversity_df[diversity_df['label_positive'] == label]['clonality'].dropna()\n",
    "            if len(subset) > 0:\n",
    "                ax3.hist(subset, bins=20, alpha=0.6, label=f'{\"Positive\" if label else \"Negative\"}', color=color)\n",
    "        ax3.legend()\n",
    "    else:\n",
    "        ax3.hist(diversity_df['clonality'].dropna(), bins=20, color='steelblue', alpha=0.7)\n",
    "ax3.set_xlabel('Clonality', fontsize=10)\n",
    "ax3.set_ylabel('Count', fontsize=10)\n",
    "ax3.set_title('Clonality Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Simpson diversity by dataset\n",
    "ax4 = axes[1, 0]\n",
    "if 'simpson_diversity' in diversity_df.columns:\n",
    "    sns.boxplot(data=diversity_df, x='dataset', y='simpson_diversity', ax=ax4)\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "ax4.set_xlabel('Dataset', fontsize=10)\n",
    "ax4.set_ylabel('Simpson Diversity', fontsize=10)\n",
    "ax4.set_title('Simpson Diversity by Dataset', fontsize=12, fontweight='bold')\n",
    "\n",
    "# V gene entropy by label\n",
    "ax5 = axes[1, 1]\n",
    "if 'v_entropy' in diversity_df.columns:\n",
    "    if diversity_df['label_positive'].notna().any():\n",
    "        sns.boxplot(data=diversity_df, x='label_positive', y='v_entropy', ax=ax5,\n",
    "                   palette={True: '#2ecc71', False: '#e74c3c'})\n",
    "ax5.set_xlabel('Label', fontsize=10)\n",
    "ax5.set_ylabel('V Gene Entropy', fontsize=10)\n",
    "ax5.set_title('V Gene Entropy by Label', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Correlation: entropy vs richness\n",
    "ax6 = axes[1, 2]\n",
    "if 'junction_entropy' in diversity_df.columns and 'junction_richness' in diversity_df.columns:\n",
    "    if diversity_df['label_positive'].notna().any():\n",
    "        for label, color in [(True, '#2ecc71'), (False, '#e74c3c')]:\n",
    "            subset = diversity_df[diversity_df['label_positive'] == label]\n",
    "            ax6.scatter(subset['junction_richness'], subset['junction_entropy'], alpha=0.5, \n",
    "                       label=f'{\"Positive\" if label else \"Negative\"}', c=color, s=30)\n",
    "        ax6.legend()\n",
    "    else:\n",
    "        ax6.scatter(diversity_df['junction_richness'], diversity_df['junction_entropy'], alpha=0.5, c='steelblue')\n",
    "ax6.set_xlabel('Richness', fontsize=10)\n",
    "ax6.set_ylabel('Shannon Entropy', fontsize=10)\n",
    "ax6.set_title('Entropy vs Richness', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nüìä Diversity Metrics Summary:\")\n",
    "diversity_cols = ['junction_entropy', 'junction_richness', 'simpson_diversity', 'clonality', 'v_entropy', 'j_entropy']\n",
    "available_cols = [c for c in diversity_cols if c in diversity_df.columns]\n",
    "display(diversity_df[available_cols].describe())\n",
    "\n",
    "# Compare by label\n",
    "if diversity_df['label_positive'].notna().any():\n",
    "    print(\"\\nüìä Diversity Metrics by Label:\")\n",
    "    display(diversity_df.groupby('label_positive')[available_cols].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Shared Sequences Analysis (\"Public Clones\" / \"Star Soldiers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SHARED SEQUENCES ANALYSIS (\"PUBLIC CLONES\" / \"STAR SOLDIERS\")\n",
    "# =============================================================================\n",
    "# Identify sequences that appear across multiple individuals - potential disease markers\n",
    "\n",
    "def find_shared_sequences(dataset_path: str, n_files: int = 50) -> Dict:\n",
    "    \"\"\"Find sequences shared across multiple repertoires.\"\"\"\n",
    "    metadata_path = os.path.join(dataset_path, 'metadata.csv')\n",
    "    \n",
    "    if os.path.exists(metadata_path):\n",
    "        metadata = pd.read_csv(metadata_path)\n",
    "        files = metadata['filename'].tolist()[:n_files]\n",
    "        labels = dict(zip(metadata['filename'], metadata['label_positive']))\n",
    "    else:\n",
    "        files = [os.path.basename(f) for f in glob.glob(os.path.join(dataset_path, \"*.tsv\"))[:n_files]]\n",
    "        labels = {}\n",
    "    \n",
    "    # Track which repertoires contain each sequence\n",
    "    seq_to_repertoires = defaultdict(set)\n",
    "    seq_to_positive = defaultdict(set)  # Track if sequence appears in positive samples\n",
    "    seq_to_negative = defaultdict(set)  # Track if sequence appears in negative samples\n",
    "    \n",
    "    for filename in tqdm(files, desc=\"Finding shared sequences\", leave=False):\n",
    "        file_path = os.path.join(dataset_path, filename)\n",
    "        label = labels.get(filename, None)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            rep = pd.read_csv(file_path, sep='\\t')\n",
    "            if 'junction_aa' in rep.columns:\n",
    "                unique_seqs = rep['junction_aa'].dropna().unique()\n",
    "                for seq in unique_seqs:\n",
    "                    seq_to_repertoires[seq].add(filename)\n",
    "                    if label is True:\n",
    "                        seq_to_positive[seq].add(filename)\n",
    "                    elif label is False:\n",
    "                        seq_to_negative[seq].add(filename)\n",
    "    \n",
    "    return {\n",
    "        'seq_to_repertoires': seq_to_repertoires,\n",
    "        'seq_to_positive': seq_to_positive,\n",
    "        'seq_to_negative': seq_to_negative,\n",
    "        'n_repertoires': len(files)\n",
    "    }\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SHARED SEQUENCES ANALYSIS (Public Clones)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze shared sequences from a sample dataset\n",
    "sample_dataset_name = train_datasets[0]\n",
    "sample_dataset_path = os.path.join(TRAIN_DIR, sample_dataset_name)\n",
    "shared_results = find_shared_sequences(sample_dataset_path, n_files=50)\n",
    "\n",
    "# Analyze sharing patterns\n",
    "sharing_counts = Counter()\n",
    "for seq, repertoires in shared_results['seq_to_repertoires'].items():\n",
    "    sharing_counts[len(repertoires)] += 1\n",
    "\n",
    "# Identify disease-associated shared sequences\n",
    "disease_enriched = []\n",
    "for seq, pos_reps in shared_results['seq_to_positive'].items():\n",
    "    neg_reps = shared_results['seq_to_negative'].get(seq, set())\n",
    "    n_pos = len(pos_reps)\n",
    "    n_neg = len(neg_reps)\n",
    "    total = n_pos + n_neg\n",
    "    if total >= 3:  # Minimum threshold\n",
    "        enrichment = n_pos / total\n",
    "        disease_enriched.append({\n",
    "            'sequence': seq,\n",
    "            'n_positive': n_pos,\n",
    "            'n_negative': n_neg,\n",
    "            'total': total,\n",
    "            'pos_ratio': enrichment\n",
    "        })\n",
    "\n",
    "disease_df = pd.DataFrame(disease_enriched)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Sharing distribution\n",
    "ax1 = axes[0, 0]\n",
    "sharing_df = pd.DataFrame.from_dict(sharing_counts, orient='index', columns=['count']).sort_index()\n",
    "ax1.bar(sharing_df.index[:20], sharing_df['count'][:20], color='steelblue', alpha=0.8)\n",
    "ax1.set_xlabel('Number of Repertoires', fontsize=10)\n",
    "ax1.set_ylabel('Number of Sequences', fontsize=10)\n",
    "ax1.set_title('Sequence Sharing Distribution', fontsize=12, fontweight='bold')\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Cumulative sharing\n",
    "ax2 = axes[0, 1]\n",
    "cumsum = sharing_df['count'].sort_index(ascending=False).cumsum()\n",
    "ax2.plot(cumsum.index, cumsum.values, color='steelblue', linewidth=2)\n",
    "ax2.set_xlabel('Minimum Number of Repertoires', fontsize=10)\n",
    "ax2.set_ylabel('Cumulative Number of Sequences', fontsize=10)\n",
    "ax2.set_title('Sequences Shared by ‚â• N Repertoires', fontsize=12, fontweight='bold')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Disease enrichment of shared sequences\n",
    "ax3 = axes[1, 0]\n",
    "if len(disease_df) > 0:\n",
    "    ax3.hist(disease_df['pos_ratio'], bins=20, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    ax3.axvline(x=0.5, color='red', linestyle='--', label='Equal distribution')\n",
    "    ax3.set_xlabel('Positive Ratio (n_pos / total)', fontsize=10)\n",
    "    ax3.set_ylabel('Count', fontsize=10)\n",
    "    ax3.set_title('Disease Enrichment of Shared Sequences', fontsize=12, fontweight='bold')\n",
    "    ax3.legend()\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'Insufficient data for analysis', ha='center', va='center', transform=ax3.transAxes)\n",
    "    ax3.axis('off')\n",
    "\n",
    "# Top disease-enriched sequences\n",
    "ax4 = axes[1, 1]\n",
    "if len(disease_df) > 0:\n",
    "    # Get top positive-enriched sequences\n",
    "    top_pos = disease_df.nlargest(15, 'pos_ratio')[['sequence', 'n_positive', 'n_negative', 'pos_ratio']]\n",
    "    \n",
    "    ax4.axis('off')\n",
    "    table = ax4.table(cellText=top_pos.round(2).values,\n",
    "                      colLabels=['Sequence', 'N Positive', 'N Negative', 'Pos Ratio'],\n",
    "                      loc='center',\n",
    "                      cellLoc='center',\n",
    "                      colColours=['#4a90d9']*4)\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(8)\n",
    "    table.scale(1.2, 1.4)\n",
    "    ax4.set_title('Top Disease-Enriched Shared Sequences', fontsize=12, fontweight='bold', y=1.02)\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'Insufficient data', ha='center', va='center', transform=ax4.transAxes)\n",
    "    ax4.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nüìä Shared Sequences Summary for {sample_dataset_name}:\")\n",
    "print(f\"   ‚Ä¢ Total unique sequences: {len(shared_results['seq_to_repertoires']):,}\")\n",
    "print(f\"   ‚Ä¢ Sequences in 1 repertoire only: {sharing_counts.get(1, 0):,}\")\n",
    "print(f\"   ‚Ä¢ Sequences in 2+ repertoires: {sum(v for k, v in sharing_counts.items() if k >= 2):,}\")\n",
    "print(f\"   ‚Ä¢ Sequences in 5+ repertoires: {sum(v for k, v in sharing_counts.items() if k >= 5):,}\")\n",
    "print(f\"   ‚Ä¢ Sequences in 10+ repertoires: {sum(v for k, v in sharing_counts.items() if k >= 10):,}\")\n",
    "\n",
    "if len(disease_df) > 0:\n",
    "    print(f\"\\nüìä Disease-Enriched Sequences (appearing in 3+ repertoires):\")\n",
    "    print(f\"   ‚Ä¢ Total: {len(disease_df):,}\")\n",
    "    print(f\"   ‚Ä¢ Highly positive-enriched (>70%): {len(disease_df[disease_df['pos_ratio'] > 0.7]):,}\")\n",
    "    print(f\"   ‚Ä¢ Highly negative-enriched (<30%): {len(disease_df[disease_df['pos_ratio'] < 0.3]):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Technical Bias Check (Batch Effects)\n",
    "\n",
    "### 6.1 Sequencing Run Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TECHNICAL BIAS CHECK - BATCH EFFECTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TECHNICAL BIAS CHECK - BATCH EFFECTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for sequencing_run_id in metadata\n",
    "if 'sequencing_run_id' in train_metadata.columns:\n",
    "    print(\"\\n‚úÖ sequencing_run_id found in metadata\")\n",
    "    \n",
    "    run_analysis = train_metadata.groupby('sequencing_run_id').agg({\n",
    "        'repertoire_id': 'count',\n",
    "        'label_positive': ['sum', 'mean'],\n",
    "        'dataset': 'nunique'\n",
    "    }).round(3)\n",
    "    run_analysis.columns = ['n_samples', 'n_positive', 'positive_rate', 'n_datasets']\n",
    "    \n",
    "    print(f\"\\nüìä Number of sequencing runs: {train_metadata['sequencing_run_id'].nunique()}\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Samples per run\n",
    "    ax1 = axes[0, 0]\n",
    "    run_counts = train_metadata['sequencing_run_id'].value_counts()\n",
    "    ax1.bar(range(len(run_counts)), run_counts.values, color='steelblue', alpha=0.7)\n",
    "    ax1.set_xlabel('Sequencing Run Index', fontsize=10)\n",
    "    ax1.set_ylabel('Number of Samples', fontsize=10)\n",
    "    ax1.set_title('Samples per Sequencing Run', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Label distribution by run\n",
    "    ax2 = axes[0, 1]\n",
    "    run_label_dist = train_metadata.groupby(['sequencing_run_id', 'label_positive']).size().unstack(fill_value=0)\n",
    "    run_label_dist.plot(kind='bar', stacked=True, ax=ax2, color=['#e74c3c', '#2ecc71'])\n",
    "    ax2.set_xlabel('Sequencing Run', fontsize=10)\n",
    "    ax2.set_ylabel('Count', fontsize=10)\n",
    "    ax2.set_title('Label Distribution by Sequencing Run', fontsize=12, fontweight='bold')\n",
    "    ax2.legend(['Negative', 'Positive'])\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Positive rate by run\n",
    "    ax3 = axes[1, 0]\n",
    "    positive_rates = train_metadata.groupby('sequencing_run_id')['label_positive'].mean().sort_values()\n",
    "    ax3.barh(range(len(positive_rates)), positive_rates.values, color='coral', alpha=0.8)\n",
    "    ax3.axvline(x=train_metadata['label_positive'].mean(), color='red', linestyle='--', \n",
    "                label=f'Overall: {train_metadata[\"label_positive\"].mean():.2f}')\n",
    "    ax3.set_xlabel('Positive Rate', fontsize=10)\n",
    "    ax3.set_ylabel('Sequencing Run', fontsize=10)\n",
    "    ax3.set_title('Positive Rate by Sequencing Run', fontsize=12, fontweight='bold')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Chi-squared test for batch effect\n",
    "    ax4 = axes[1, 1]\n",
    "    contingency = pd.crosstab(train_metadata['sequencing_run_id'], train_metadata['label_positive'])\n",
    "    chi2, p_val, dof, expected = stats.chi2_contingency(contingency)\n",
    "    \n",
    "    ax4.axis('off')\n",
    "    text = f\"\"\"BATCH EFFECT STATISTICAL TEST\n",
    "    \n",
    "Chi-squared Test Results:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "œá¬≤ statistic: {chi2:.2f}\n",
    "Degrees of freedom: {dof}\n",
    "P-value: {p_val:.2e}\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "Interpretation:\n",
    "{'‚ö†Ô∏è SIGNIFICANT batch effect detected!' if p_val < 0.05 else '‚úÖ No significant batch effect'}\n",
    "{'Consider stratified sampling by run' if p_val < 0.05 else 'Labels are independent of sequencing run'}\n",
    "\"\"\"\n",
    "    ax4.text(0.1, 0.5, text, fontsize=11, family='monospace', transform=ax4.transAxes, \n",
    "             verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Sequencing Run Summary:\")\n",
    "    display(run_analysis.sort_values('n_samples', ascending=False).head(15))\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è sequencing_run_id not found in metadata\")\n",
    "    print(\"   Batch effect analysis by sequencing run not possible.\")\n",
    "    \n",
    "    # Check for other potential batch indicators\n",
    "    potential_batch_cols = ['study_group', 'cohort', 'batch', 'plate', 'run']\n",
    "    found_cols = [col for col in potential_batch_cols if col in train_metadata.columns]\n",
    "    \n",
    "    if found_cols:\n",
    "        print(f\"\\n   Alternative batch indicators found: {found_cols}\")\n",
    "        for col in found_cols:\n",
    "            print(f\"\\nüìä {col} distribution:\")\n",
    "            print(train_metadata[col].value_counts().head(10))\n",
    "    else:\n",
    "        print(\"   No alternative batch indicators found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. HLA Gene Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HLA GENE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"HLA GENE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find HLA-related columns\n",
    "hla_cols = [col for col in train_metadata.columns if 'hla' in col.lower() or 'mhc' in col.lower()]\n",
    "\n",
    "if hla_cols:\n",
    "    print(f\"\\n‚úÖ HLA-related columns found: {hla_cols}\")\n",
    "    \n",
    "    for hla_col in hla_cols:\n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(f\"ANALYSIS OF: {hla_col}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Basic statistics\n",
    "        n_unique = train_metadata[hla_col].nunique()\n",
    "        n_missing = train_metadata[hla_col].isna().sum()\n",
    "        print(f\"\\nüìä Basic stats:\")\n",
    "        print(f\"   ‚Ä¢ Unique values: {n_unique:,}\")\n",
    "        print(f\"   ‚Ä¢ Missing: {n_missing:,} ({n_missing/len(train_metadata)*100:.1f}%)\")\n",
    "        \n",
    "        if n_unique > 0 and n_unique < 100:\n",
    "            # Distribution plot\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            # Overall distribution\n",
    "            ax1 = axes[0]\n",
    "            value_counts = train_metadata[hla_col].value_counts().head(20)\n",
    "            ax1.barh(value_counts.index.astype(str), value_counts.values, color='steelblue', alpha=0.8)\n",
    "            ax1.set_xlabel('Count', fontsize=10)\n",
    "            ax1.set_ylabel(hla_col, fontsize=10)\n",
    "            ax1.set_title(f'Top 20 {hla_col} Values', fontsize=12, fontweight='bold')\n",
    "            ax1.invert_yaxis()\n",
    "            \n",
    "            # Association with label\n",
    "            ax2 = axes[1]\n",
    "            if 'label_positive' in train_metadata.columns:\n",
    "                # Get top HLA types and their positive rates\n",
    "                hla_label_stats = train_metadata.groupby(hla_col).agg({\n",
    "                    'label_positive': ['count', 'sum', 'mean']\n",
    "                }).round(3)\n",
    "                hla_label_stats.columns = ['count', 'n_positive', 'positive_rate']\n",
    "                hla_label_stats = hla_label_stats[hla_label_stats['count'] >= 5].sort_values('positive_rate', ascending=False)\n",
    "                \n",
    "                if len(hla_label_stats) > 0:\n",
    "                    top_hla = hla_label_stats.head(15)\n",
    "                    colors = ['#2ecc71' if r > 0.5 else '#e74c3c' for r in top_hla['positive_rate']]\n",
    "                    ax2.barh(top_hla.index.astype(str), top_hla['positive_rate'], color=colors, alpha=0.8)\n",
    "                    ax2.axvline(x=train_metadata['label_positive'].mean(), color='black', linestyle='--', \n",
    "                               label=f'Overall rate: {train_metadata[\"label_positive\"].mean():.2f}')\n",
    "                    ax2.set_xlabel('Positive Rate', fontsize=10)\n",
    "                    ax2.set_ylabel(hla_col, fontsize=10)\n",
    "                    ax2.set_title(f'{hla_col} Association with Label (min 5 samples)', fontsize=12, fontweight='bold')\n",
    "                    ax2.legend()\n",
    "                    ax2.invert_yaxis()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Statistical test for association\n",
    "            if 'label_positive' in train_metadata.columns:\n",
    "                # Chi-squared test\n",
    "                valid_data = train_metadata[[hla_col, 'label_positive']].dropna()\n",
    "                if len(valid_data) > 0:\n",
    "                    contingency = pd.crosstab(valid_data[hla_col], valid_data['label_positive'])\n",
    "                    if contingency.shape[0] > 1 and contingency.shape[1] > 1:\n",
    "                        chi2, p_val, dof, expected = stats.chi2_contingency(contingency)\n",
    "                        print(f\"\\nüìä Chi-squared test for {hla_col} vs label:\")\n",
    "                        print(f\"   ‚Ä¢ œá¬≤ = {chi2:.2f}\")\n",
    "                        print(f\"   ‚Ä¢ p-value = {p_val:.2e}\")\n",
    "                        print(f\"   ‚Ä¢ {'‚ö†Ô∏è SIGNIFICANT association!' if p_val < 0.05 else '‚úÖ No significant association'}\")\n",
    "        \n",
    "        elif n_unique >= 100:\n",
    "            print(f\"\\n‚ö†Ô∏è Too many unique values ({n_unique}) for detailed visualization\")\n",
    "            print(\"   Top 10 values:\")\n",
    "            print(train_metadata[hla_col].value_counts().head(10))\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No HLA-related columns found in metadata\")\n",
    "    print(\"   HLA analysis not possible. Columns available:\")\n",
    "    print(f\"   {list(train_metadata.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Missing Values Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MISSING VALUES ASSESSMENT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MISSING VALUES ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Metadata missing values\n",
    "print(\"\\nüìã METADATA MISSING VALUES:\")\n",
    "missing_meta = train_metadata.isnull().sum()\n",
    "missing_pct = (missing_meta / len(train_metadata) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_meta.index,\n",
    "    'Missing Count': missing_meta.values,\n",
    "    'Missing %': missing_pct.values,\n",
    "    'Total': len(train_metadata)\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing %', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Missing values bar chart\n",
    "    ax1 = axes[0]\n",
    "    colors = ['#e74c3c' if pct > 50 else '#f39c12' if pct > 20 else '#2ecc71' for pct in missing_df['Missing %']]\n",
    "    ax1.barh(missing_df['Column'], missing_df['Missing %'], color=colors, alpha=0.8)\n",
    "    ax1.set_xlabel('Missing %', fontsize=10)\n",
    "    ax1.set_ylabel('Column', fontsize=10)\n",
    "    ax1.set_title('Missing Values in Metadata', fontsize=12, fontweight='bold')\n",
    "    ax1.axvline(x=50, color='red', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "    ax1.axvline(x=20, color='orange', linestyle='--', alpha=0.5, label='20% threshold')\n",
    "    ax1.legend(fontsize=8)\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # Missing values by dataset\n",
    "    ax2 = axes[1]\n",
    "    dataset_missing = train_metadata.groupby('dataset').apply(lambda x: x.isnull().sum().sum() / (len(x) * len(x.columns)) * 100)\n",
    "    dataset_missing = dataset_missing.sort_values(ascending=False)\n",
    "    ax2.barh(dataset_missing.index, dataset_missing.values, color='steelblue', alpha=0.8)\n",
    "    ax2.set_xlabel('Overall Missing %', fontsize=10)\n",
    "    ax2.set_ylabel('Dataset', fontsize=10)\n",
    "    ax2.set_title('Missing Values by Dataset', fontsize=12, fontweight='bold')\n",
    "    ax2.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Missing Values Summary:\")\n",
    "    display(missing_df)\n",
    "else:\n",
    "    print(\"   ‚úÖ No missing values in metadata!\")\n",
    "\n",
    "# Check for -999.0 placeholder values (as mentioned in competition description)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"üìã CHECKING FOR -999.0 PLACEHOLDER VALUES:\")\n",
    "for col in train_metadata.select_dtypes(include=[np.number]).columns:\n",
    "    n_placeholder = (train_metadata[col] == -999.0).sum()\n",
    "    if n_placeholder > 0:\n",
    "        print(f\"   ‚Ä¢ {col}: {n_placeholder:,} instances of -999.0\")\n",
    "\n",
    "# Repertoire-level missing values analysis\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"üìã REPERTOIRE-LEVEL MISSING VALUES (Sample):\")\n",
    "\n",
    "# Check a few repertoire files\n",
    "sample_dataset_path = os.path.join(TRAIN_DIR, train_datasets[0])\n",
    "sample_files = glob.glob(os.path.join(sample_dataset_path, \"*.tsv\"))[:5]\n",
    "\n",
    "repertoire_missing = []\n",
    "for file_path in sample_files:\n",
    "    rep = pd.read_csv(file_path, sep='\\t')\n",
    "    missing_info = {\n",
    "        'file': os.path.basename(file_path),\n",
    "        'total_rows': len(rep)\n",
    "    }\n",
    "    for col in rep.columns:\n",
    "        missing_info[f'{col}_missing'] = rep[col].isnull().sum()\n",
    "        missing_info[f'{col}_missing_%'] = rep[col].isnull().sum() / len(rep) * 100\n",
    "    repertoire_missing.append(missing_info)\n",
    "\n",
    "rep_missing_df = pd.DataFrame(repertoire_missing)\n",
    "print(f\"\\nSample repertoire files from {train_datasets[0]}:\")\n",
    "\n",
    "# Show missing percentages for key columns\n",
    "key_cols = ['junction_aa', 'v_call', 'j_call']\n",
    "for col in key_cols:\n",
    "    if f'{col}_missing_%' in rep_missing_df.columns:\n",
    "        avg_missing = rep_missing_df[f'{col}_missing_%'].mean()\n",
    "        max_missing = rep_missing_df[f'{col}_missing_%'].max()\n",
    "        print(f\"   ‚Ä¢ {col}: avg {avg_missing:.1f}% missing, max {max_missing:.1f}% missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train vs Test Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN VS TEST COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAIN VS TEST COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get test dataset info\n",
    "test_datasets = sorted([d for d in os.listdir(TEST_DIR) if d.startswith(\"test_dataset_\")])\n",
    "\n",
    "print(f\"\\nüìä Overview:\")\n",
    "print(f\"   ‚Ä¢ Training datasets: {len(train_datasets)}\")\n",
    "print(f\"   ‚Ä¢ Test datasets: {len(test_datasets)}\")\n",
    "\n",
    "# Map test datasets to their training counterparts\n",
    "train_test_mapping = defaultdict(list)\n",
    "for test_ds in test_datasets:\n",
    "    # Extract base ID (e.g., \"test_dataset_1_a\" -> \"1\")\n",
    "    base_id = test_ds.replace(\"test_dataset_\", \"\").split(\"_\")[0]\n",
    "    train_name = f\"train_dataset_{base_id}\"\n",
    "    if train_name in train_datasets:\n",
    "        train_test_mapping[train_name].append(test_ds)\n",
    "\n",
    "print(f\"\\nüìã Train-Test Dataset Mapping:\")\n",
    "for train_ds, test_list in list(train_test_mapping.items())[:10]:\n",
    "    print(f\"   {train_ds} ‚Üí {test_list}\")\n",
    "if len(train_test_mapping) > 10:\n",
    "    print(f\"   ... and {len(train_test_mapping) - 10} more\")\n",
    "\n",
    "# Analyze test data structure\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST DATA STRUCTURE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def get_dataset_stats(base_dir: str, dataset_name: str) -> Dict:\n",
    "    \"\"\"Get basic stats for a dataset.\"\"\"\n",
    "    dataset_path = os.path.join(base_dir, dataset_name)\n",
    "    tsv_files = glob.glob(os.path.join(dataset_path, \"*.tsv\"))\n",
    "    \n",
    "    stats = {\n",
    "        'dataset': dataset_name,\n",
    "        'n_files': len(tsv_files),\n",
    "        'has_metadata': os.path.exists(os.path.join(dataset_path, 'metadata.csv'))\n",
    "    }\n",
    "    \n",
    "    # Sample a file to get column info\n",
    "    if tsv_files:\n",
    "        sample_file = pd.read_csv(tsv_files[0], sep='\\t', nrows=10)\n",
    "        stats['columns'] = list(sample_file.columns)\n",
    "        stats['n_columns'] = len(sample_file.columns)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Compare train and test structure\n",
    "comparison_data = []\n",
    "\n",
    "for train_ds in train_datasets[:5]:  # Sample comparison\n",
    "    train_stats = get_dataset_stats(TRAIN_DIR, train_ds)\n",
    "    train_stats['type'] = 'train'\n",
    "    comparison_data.append(train_stats)\n",
    "    \n",
    "    for test_ds in train_test_mapping.get(train_ds, [])[:2]:\n",
    "        test_stats = get_dataset_stats(TEST_DIR, test_ds)\n",
    "        test_stats['type'] = 'test'\n",
    "        comparison_data.append(test_stats)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Number of repertoires\n",
    "ax1 = axes[0, 0]\n",
    "train_counts = train_metadata.groupby('dataset').size()\n",
    "test_counts = []\n",
    "for test_ds in test_datasets[:len(train_datasets)]:\n",
    "    test_path = os.path.join(TEST_DIR, test_ds)\n",
    "    n_files = len(glob.glob(os.path.join(test_path, \"*.tsv\")))\n",
    "    test_counts.append({'dataset': test_ds, 'count': n_files})\n",
    "test_counts_df = pd.DataFrame(test_counts)\n",
    "\n",
    "x = np.arange(min(10, len(train_counts)))\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, train_counts.values[:10], width, label='Train', color='steelblue', alpha=0.8)\n",
    "if len(test_counts_df) > 0:\n",
    "    ax1.bar(x + width/2, test_counts_df['count'].values[:10], width, label='Test', color='coral', alpha=0.8)\n",
    "ax1.set_xlabel('Dataset Index', fontsize=10)\n",
    "ax1.set_ylabel('Number of Repertoires', fontsize=10)\n",
    "ax1.set_title('Repertoire Count: Train vs Test', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "\n",
    "# Column comparison\n",
    "ax2 = axes[0, 1]\n",
    "if 'columns' in comparison_df.columns:\n",
    "    train_cols = set()\n",
    "    test_cols = set()\n",
    "    for _, row in comparison_df.iterrows():\n",
    "        if row['type'] == 'train':\n",
    "            train_cols.update(row['columns'])\n",
    "        else:\n",
    "            test_cols.update(row['columns'])\n",
    "    \n",
    "    common = train_cols & test_cols\n",
    "    train_only = train_cols - test_cols\n",
    "    test_only = test_cols - train_cols\n",
    "    \n",
    "    data = [len(common), len(train_only), len(test_only)]\n",
    "    labels = ['Common', 'Train Only', 'Test Only']\n",
    "    colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "    ax2.pie(data, labels=labels, autopct='%1.0f', colors=colors, explode=(0.02, 0.02, 0.02))\n",
    "    ax2.set_title('Column Overlap: Train vs Test', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    print(\"\\nüìã Column Comparison:\")\n",
    "    print(f\"   ‚Ä¢ Common columns: {common}\")\n",
    "    print(f\"   ‚Ä¢ Train-only columns: {train_only}\")\n",
    "    print(f\"   ‚Ä¢ Test-only columns: {test_only}\")\n",
    "\n",
    "# Summary table\n",
    "ax3 = axes[1, 0]\n",
    "ax3.axis('off')\n",
    "summary_text = f\"\"\"\n",
    "TRAIN VS TEST SUMMARY\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "Training Data:\n",
    "  ‚Ä¢ Datasets: {len(train_datasets)}\n",
    "  ‚Ä¢ Total repertoires: {len(train_metadata):,}\n",
    "  ‚Ä¢ Has labels: Yes (label_positive)\n",
    "\n",
    "Test Data:\n",
    "  ‚Ä¢ Datasets: {len(test_datasets)}\n",
    "  ‚Ä¢ Has labels: No (to be predicted)\n",
    "  ‚Ä¢ Mapped to {len(train_test_mapping)} training datasets\n",
    "\n",
    "Key Differences:\n",
    "  ‚Ä¢ Test data has NO label_positive column\n",
    "  ‚Ä¢ Predictions needed for {sum(test_counts_df['count']) if len(test_counts_df) > 0 else 'N/A'} test repertoires\n",
    "\"\"\"\n",
    "ax3.text(0.1, 0.5, summary_text, fontsize=11, family='monospace', transform=ax3.transAxes,\n",
    "         verticalalignment='center', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "# Repertoire sizes comparison\n",
    "ax4 = axes[1, 1]\n",
    "# Sample test repertoire sizes\n",
    "test_sizes = []\n",
    "for test_ds in test_datasets[:3]:\n",
    "    test_path = os.path.join(TEST_DIR, test_ds)\n",
    "    for file_path in glob.glob(os.path.join(test_path, \"*.tsv\"))[:10]:\n",
    "        with open(file_path, 'r') as f:\n",
    "            n_lines = sum(1 for _ in f) - 1\n",
    "        test_sizes.append({'dataset': test_ds, 'n_sequences': n_lines, 'type': 'test'})\n",
    "\n",
    "# Compare with train sizes\n",
    "if not repertoire_sizes.empty:\n",
    "    train_size_sample = repertoire_sizes.head(100).copy()\n",
    "    train_size_sample['type'] = 'train'\n",
    "    combined_sizes = pd.concat([train_size_sample[['n_sequences', 'type']], \n",
    "                                pd.DataFrame(test_sizes)[['n_sequences', 'type']]], ignore_index=True)\n",
    "    sns.boxplot(data=combined_sizes, x='type', y='n_sequences', ax=ax4, palette=['steelblue', 'coral'])\n",
    "    ax4.set_xlabel('Dataset Type', fontsize=10)\n",
    "    ax4.set_ylabel('Number of Sequences', fontsize=10)\n",
    "    ax4.set_title('Repertoire Size: Train vs Test', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Dimensionality Reduction Visualization (PCA/t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DIMENSIONALITY REDUCTION VISUALIZATION\n",
    "# =============================================================================\n",
    "# Create k-mer features for repertoires and visualize with PCA/t-SNE\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def compute_kmer_features(dataset_path: str, k: int = 3, n_files: int = 50) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Compute k-mer frequency features for repertoires.\"\"\"\n",
    "    metadata_path = os.path.join(dataset_path, 'metadata.csv')\n",
    "    \n",
    "    if os.path.exists(metadata_path):\n",
    "        metadata = pd.read_csv(metadata_path)\n",
    "        files = metadata['filename'].tolist()[:n_files]\n",
    "        labels = dict(zip(metadata['filename'], metadata['label_positive']))\n",
    "    else:\n",
    "        files = [os.path.basename(f) for f in glob.glob(os.path.join(dataset_path, \"*.tsv\"))[:n_files]]\n",
    "        labels = {}\n",
    "    \n",
    "    feature_list = []\n",
    "    meta_list = []\n",
    "    \n",
    "    for filename in tqdm(files, desc=\"Computing k-mer features\", leave=False):\n",
    "        file_path = os.path.join(dataset_path, filename)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            rep = pd.read_csv(file_path, sep='\\t')\n",
    "            \n",
    "            if 'junction_aa' in rep.columns:\n",
    "                kmer_counts = Counter()\n",
    "                for seq in rep['junction_aa'].dropna():\n",
    "                    for i in range(len(seq) - k + 1):\n",
    "                        kmer_counts[seq[i:i+k]] += 1\n",
    "                \n",
    "                # Normalize by total\n",
    "                total = sum(kmer_counts.values())\n",
    "                kmer_freq = {kmer: count/total for kmer, count in kmer_counts.items()}\n",
    "                kmer_freq['filename'] = filename\n",
    "                feature_list.append(kmer_freq)\n",
    "                \n",
    "                meta_list.append({\n",
    "                    'filename': filename,\n",
    "                    'label_positive': labels.get(filename, None),\n",
    "                    'n_sequences': len(rep)\n",
    "                })\n",
    "    \n",
    "    features_df = pd.DataFrame(feature_list).fillna(0).set_index('filename')\n",
    "    meta_df = pd.DataFrame(meta_list)\n",
    "    \n",
    "    return features_df, meta_df\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DIMENSIONALITY REDUCTION VISUALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compute k-mer features for a sample dataset\n",
    "sample_dataset_name = train_datasets[0]\n",
    "sample_dataset_path = os.path.join(TRAIN_DIR, sample_dataset_name)\n",
    "\n",
    "print(f\"\\nüìä Computing 3-mer features for {sample_dataset_name}...\")\n",
    "kmer_features, kmer_meta = compute_kmer_features(sample_dataset_path, k=3, n_files=100)\n",
    "\n",
    "if len(kmer_features) > 10:\n",
    "    # Select top N most variable k-mers\n",
    "    kmer_var = kmer_features.var().sort_values(ascending=False)\n",
    "    top_kmers = kmer_var.head(500).index\n",
    "    X = kmer_features[top_kmers].values\n",
    "    \n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # PCA\n",
    "    print(\"   Computing PCA...\")\n",
    "    pca = PCA(n_components=min(50, X_scaled.shape[1]))\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # t-SNE on PCA-reduced features\n",
    "    print(\"   Computing t-SNE...\")\n",
    "    tsne = TSNE(n_components=2, perplexity=min(30, len(X_pca)//4), random_state=42, n_iter=1000)\n",
    "    X_tsne = tsne.fit_transform(X_pca[:, :min(30, X_pca.shape[1])])\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    # PCA variance explained\n",
    "    ax1 = axes[0, 0]\n",
    "    cumsum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "    ax1.plot(range(1, len(cumsum_var)+1), cumsum_var, 'b-', linewidth=2)\n",
    "    ax1.axhline(y=0.9, color='r', linestyle='--', label='90% variance')\n",
    "    ax1.axhline(y=0.95, color='orange', linestyle='--', label='95% variance')\n",
    "    ax1.set_xlabel('Number of Components', fontsize=10)\n",
    "    ax1.set_ylabel('Cumulative Explained Variance', fontsize=10)\n",
    "    ax1.set_title('PCA Explained Variance', fontsize=12, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # PCA plot colored by label\n",
    "    ax2 = axes[0, 1]\n",
    "    labels_array = kmer_meta.set_index('filename').loc[kmer_features.index, 'label_positive'].values\n",
    "    if pd.notna(labels_array).any():\n",
    "        scatter = ax2.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                             c=['#2ecc71' if l == True else '#e74c3c' if l == False else 'gray' for l in labels_array],\n",
    "                             alpha=0.6, s=50)\n",
    "        ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=10)\n",
    "        ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=10)\n",
    "        ax2.set_title('PCA: Colored by Label', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Add legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [Patch(facecolor='#2ecc71', label='Positive'),\n",
    "                          Patch(facecolor='#e74c3c', label='Negative')]\n",
    "        ax2.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    # t-SNE colored by label\n",
    "    ax3 = axes[1, 0]\n",
    "    if pd.notna(labels_array).any():\n",
    "        scatter = ax3.scatter(X_tsne[:, 0], X_tsne[:, 1],\n",
    "                             c=['#2ecc71' if l == True else '#e74c3c' if l == False else 'gray' for l in labels_array],\n",
    "                             alpha=0.6, s=50)\n",
    "        ax3.set_xlabel('t-SNE 1', fontsize=10)\n",
    "        ax3.set_ylabel('t-SNE 2', fontsize=10)\n",
    "        ax3.set_title('t-SNE: Colored by Label', fontsize=12, fontweight='bold')\n",
    "        ax3.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    # t-SNE colored by repertoire size\n",
    "    ax4 = axes[1, 1]\n",
    "    sizes_array = kmer_meta.set_index('filename').loc[kmer_features.index, 'n_sequences'].values\n",
    "    scatter = ax4.scatter(X_tsne[:, 0], X_tsne[:, 1], c=np.log10(sizes_array), \n",
    "                         cmap='viridis', alpha=0.6, s=50)\n",
    "    plt.colorbar(scatter, ax=ax4, label='log10(n_sequences)')\n",
    "    ax4.set_xlabel('t-SNE 1', fontsize=10)\n",
    "    ax4.set_ylabel('t-SNE 2', fontsize=10)\n",
    "    ax4.set_title('t-SNE: Colored by Repertoire Size', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print insights\n",
    "    print(f\"\\nüìä Dimensionality Reduction Results:\")\n",
    "    print(f\"   ‚Ä¢ Number of repertoires: {len(kmer_features)}\")\n",
    "    print(f\"   ‚Ä¢ Number of k-mer features: {len(top_kmers)}\")\n",
    "    print(f\"   ‚Ä¢ Components for 90% variance: {np.argmax(cumsum_var >= 0.9) + 1}\")\n",
    "    print(f\"   ‚Ä¢ Components for 95% variance: {np.argmax(cumsum_var >= 0.95) + 1}\")\n",
    "    \n",
    "    # Check for separation\n",
    "    if pd.notna(labels_array).any():\n",
    "        pos_mask = labels_array == True\n",
    "        neg_mask = labels_array == False\n",
    "        if pos_mask.sum() > 0 and neg_mask.sum() > 0:\n",
    "            from scipy.stats import mannwhitneyu\n",
    "            # Test separation on PC1\n",
    "            stat, pval = mannwhitneyu(X_pca[pos_mask, 0], X_pca[neg_mask, 0])\n",
    "            print(f\"\\nüìä Separability Test (PC1):\")\n",
    "            print(f\"   ‚Ä¢ Mann-Whitney U test p-value: {pval:.2e}\")\n",
    "            print(f\"   ‚Ä¢ {'‚úÖ Significant separation!' if pval < 0.05 else '‚ö†Ô∏è No significant separation'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough data for dimensionality reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. EDA Summary & Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EDA SUMMARY & KEY FINDINGS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä EDA SUMMARY & KEY FINDINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_text = \"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                          DATA OVERVIEW                                       ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ Training Data:                                                               ‚îÇ\n",
    "‚îÇ   ‚Ä¢ Number of datasets: {n_train_datasets}                                   ‚îÇ\n",
    "‚îÇ   ‚Ä¢ Total repertoires: {n_train_samples:,}                                   ‚îÇ\n",
    "‚îÇ   ‚Ä¢ Class balance: {pos_pct:.1f}% positive, {neg_pct:.1f}% negative          ‚îÇ\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îÇ Test Data:                                                                   ‚îÇ\n",
    "‚îÇ   ‚Ä¢ Number of datasets: {n_test_datasets}                                    ‚îÇ\n",
    "‚îÇ   ‚Ä¢ Labels: Not provided (to predict)                                        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                          KEY FINDINGS                                        ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ 1. METADATA FEATURES:                                                        ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Available columns vary by dataset                                       ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Demographic features may have significant missing values                ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Some features show association with label (check correlation results)   ‚îÇ\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îÇ 2. SEQUENCE CHARACTERISTICS:                                                 ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Junction AA lengths vary across datasets                                ‚îÇ\n",
    "‚îÇ    ‚Ä¢ V/J gene usage patterns differ between datasets                         ‚îÇ\n",
    "‚îÇ    ‚Ä¢ D gene information may not be available in all datasets                 ‚îÇ\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îÇ 3. DIVERSITY METRICS:                                                        ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Shannon entropy and clonality vary by label                             ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Disease samples may show altered diversity patterns                     ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Consider diversity as a feature for classification                      ‚îÇ\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îÇ 4. SHARED SEQUENCES (PUBLIC CLONES):                                         ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Many sequences are private (appear in single individual)                ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Some sequences appear across multiple individuals                       ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Disease-associated public clones may exist                              ‚îÇ\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îÇ 5. POTENTIAL ISSUES:                                                         ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Check for batch effects if sequencing_run_id varies                     ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Missing values need handling strategy                                   ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Dataset heterogeneity requires careful cross-validation                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                     RECOMMENDATIONS FOR MODELING                             ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ 1. Feature Engineering:                                                      ‚îÇ\n",
    "‚îÇ    ‚Ä¢ K-mer frequencies (3-mer, 4-mer)                                        ‚îÇ\n",
    "‚îÇ    ‚Ä¢ V/J gene usage profiles                                                 ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Diversity metrics (entropy, clonality, richness)                        ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Sequence length distributions                                           ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Public clone presence/absence                                           ‚îÇ\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îÇ 2. Model Selection:                                                          ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Consider ensemble methods for heterogeneous data                        ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Test both traditional ML (XGBoost, RF) and deep learning                ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Use stratified cross-validation by dataset                              ‚îÇ\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îÇ 3. Important Sequence Identification:                                        ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Fisher's exact test for sequence-label association                      ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Feature importance from tree-based models                               ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Attention weights from neural networks                                  ‚îÇ\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îÇ 4. Handling Challenges:                                                      ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Impute or handle missing values consistently                            ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Account for batch effects in validation                                 ‚îÇ\n",
    "‚îÇ    ‚Ä¢ Normalize for repertoire size differences                               ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\"\n",
    "\n",
    "# Calculate summary statistics\n",
    "if 'label_positive' in train_metadata.columns:\n",
    "    pos_count = train_metadata['label_positive'].sum()\n",
    "    neg_count = len(train_metadata) - pos_count\n",
    "    pos_pct = pos_count / len(train_metadata) * 100\n",
    "    neg_pct = neg_count / len(train_metadata) * 100\n",
    "else:\n",
    "    pos_pct = neg_pct = 0\n",
    "\n",
    "print(summary_text.format(\n",
    "    n_train_datasets=len(train_datasets),\n",
    "    n_train_samples=len(train_metadata),\n",
    "    pos_pct=pos_pct,\n",
    "    neg_pct=neg_pct,\n",
    "    n_test_datasets=len(test_datasets)\n",
    "))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ EDA COMPLETE - Ready for modeling!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Model Template & Submission Code\n",
    "\n",
    "The following sections contain the competition template code for building the `ImmuneStatePredictor` class and generating submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need for a uniform interface of running models\n",
    "\n",
    "As described in the official competition page, to win the prize money, a prerequisite is that the code has to be made open-source. In addition, the top 10 submissions/teams will be invited to become co-authors in a scientific paper that involves further stress-testing of their models in a subsequent phase with many other datasets outside Kaggle platform. **To enable such further analyses and re-use of the models by the community, we strongly encourage** the participants to adhere to a code template that we provide through this repository that enables a uniform interface of running models: [https://github.com/uio-bmi/predict-airr](https://github.com/uio-bmi/predict-airr)\n",
    "\n",
    "\n",
    "Ideally, all the methods can be run in a unified way, e.g.,\n",
    "\n",
    "`python3 -m submission.main --train_dir /path/to/train_dir --test_dirs /path/to/test_dir_1 /path/to/test_dir_2 --out_dir /path/to/output_dir --n_jobs 4 --device cpu`\n",
    "\n",
    "## Adhering to code template on Kaggle Notebooks\n",
    "\n",
    "Those participants who make use of Kaggle resources and Kaggle notebooks to develop and run their code are also strongly encouraged to copy the code template, particularly the `ImmuneStatePredictor` class and any utility functions from the provided code template repository and adhere to the code template to enable a unified way of running different methods at a later stage. In this notebook, we copied the code template below for participants to paste into their respective Kaggle notebooks and edit as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## imports required for the basic code template below.\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import sys\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "from typing import Iterator, Tuple, Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## some utility functions such as data loaders, etc.\n",
    "\n",
    "def load_data_generator(data_dir: str, metadata_filename='metadata.csv') -> Iterator[\n",
    "    Union[Tuple[str, pd.DataFrame, bool], Tuple[str, pd.DataFrame]]]:\n",
    "    \"\"\"\n",
    "    A generator to load immune repertoire data.\n",
    "\n",
    "    This function operates in two modes:\n",
    "    1.  If metadata is found, it yields data based on the metadata file.\n",
    "    2.  If metadata is NOT found, it uses glob to find and yield all '.tsv'\n",
    "        files in the directory.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): The path to the directory containing the data.\n",
    "\n",
    "    Yields:\n",
    "        An iterator of tuples. The format depends on the mode:\n",
    "        - With metadata: (repertoire_id, pd.DataFrame, label_positive)\n",
    "        - Without metadata: (filename, pd.DataFrame)\n",
    "    \"\"\"\n",
    "    metadata_path = os.path.join(data_dir, metadata_filename)\n",
    "\n",
    "    if os.path.exists(metadata_path):\n",
    "        metadata_df = pd.read_csv(metadata_path)\n",
    "        for row in metadata_df.itertuples(index=False):\n",
    "            file_path = os.path.join(data_dir, row.filename)\n",
    "            try:\n",
    "                repertoire_df = pd.read_csv(file_path, sep='\\t')\n",
    "                yield row.repertoire_id, repertoire_df, row.label_positive\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: File '{row.filename}' listed in metadata not found. Skipping.\")\n",
    "                continue\n",
    "    else:\n",
    "        search_pattern = os.path.join(data_dir, '*.tsv')\n",
    "        tsv_files = glob.glob(search_pattern)\n",
    "        for file_path in sorted(tsv_files):\n",
    "            try:\n",
    "                filename = os.path.basename(file_path)\n",
    "                repertoire_df = pd.read_csv(file_path, sep='\\t')\n",
    "                yield filename, repertoire_df\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read file '{file_path}'. Error: {e}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "\n",
    "def load_full_dataset(data_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads all TSV files from a directory and concatenates them into a single DataFrame.\n",
    "\n",
    "    This function handles two scenarios:\n",
    "    1. If metadata.csv exists, it loads data based on the metadata and adds\n",
    "       'repertoire_id' and 'label_positive' columns.\n",
    "    2. If metadata.csv does not exist, it loads all .tsv files and adds\n",
    "       a 'filename' column as an identifier.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): The path to the data directory.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A single, concatenated DataFrame containing all the data.\n",
    "    \"\"\"\n",
    "    metadata_path = os.path.join(data_dir, 'metadata.csv')\n",
    "    df_list = []\n",
    "    data_loader = load_data_generator(data_dir=data_dir)\n",
    "\n",
    "    if os.path.exists(metadata_path):\n",
    "        metadata_df = pd.read_csv(metadata_path)\n",
    "        total_files = len(metadata_df)\n",
    "        for rep_id, data_df, label in tqdm(data_loader, total=total_files, desc=\"Loading files\"):\n",
    "            data_df['ID'] = rep_id\n",
    "            data_df['label_positive'] = label\n",
    "            df_list.append(data_df)\n",
    "    else:\n",
    "        search_pattern = os.path.join(data_dir, '*.tsv')\n",
    "        total_files = len(glob.glob(search_pattern))\n",
    "        for filename, data_df in tqdm(data_loader, total=total_files, desc=\"Loading files\"):\n",
    "            data_df['ID'] = os.path.basename(filename).replace(\".tsv\", \"\")\n",
    "            df_list.append(data_df)\n",
    "\n",
    "    if not df_list:\n",
    "        print(\"Warning: No data files were loaded.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    full_dataset_df = pd.concat(df_list, ignore_index=True)\n",
    "    return full_dataset_df\n",
    "\n",
    "\n",
    "def load_and_encode_kmers(data_dir: str, k: int = 3) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Loading and k-mer encoding of repertoire data.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Path to data directory\n",
    "        k: K-mer length\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (encoded_features_df, metadata_df)\n",
    "        metadata_df always contains 'ID', and 'label_positive' if available\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    metadata_path = os.path.join(data_dir, 'metadata.csv')\n",
    "    data_loader = load_data_generator(data_dir=data_dir)\n",
    "\n",
    "    repertoire_features = []\n",
    "    metadata_records = []\n",
    "\n",
    "    search_pattern = os.path.join(data_dir, '*.tsv')\n",
    "    total_files = len(glob.glob(search_pattern))\n",
    "\n",
    "    for item in tqdm(data_loader, total=total_files, desc=f\"Encoding {k}-mers\"):\n",
    "        if os.path.exists(metadata_path):\n",
    "            rep_id, data_df, label = item\n",
    "        else:\n",
    "            filename, data_df = item\n",
    "            rep_id = os.path.basename(filename).replace(\".tsv\", \"\")\n",
    "            label = None\n",
    "\n",
    "        kmer_counts = Counter()\n",
    "        for seq in data_df['junction_aa'].dropna():\n",
    "            for i in range(len(seq) - k + 1):\n",
    "                kmer_counts[seq[i:i + k]] += 1\n",
    "\n",
    "        repertoire_features.append({\n",
    "            'ID': rep_id,\n",
    "            **kmer_counts\n",
    "        })\n",
    "\n",
    "        metadata_record = {'ID': rep_id}\n",
    "        if label is not None:\n",
    "            metadata_record['label_positive'] = label\n",
    "        metadata_records.append(metadata_record)\n",
    "\n",
    "        del data_df, kmer_counts\n",
    "\n",
    "    features_df = pd.DataFrame(repertoire_features).fillna(0).set_index('ID')\n",
    "    features_df.fillna(0)\n",
    "    metadata_df = pd.DataFrame(metadata_records)\n",
    "\n",
    "    return features_df, metadata_df\n",
    "\n",
    "\n",
    "def save_tsv(df: pd.DataFrame, path: str):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    df.to_csv(path, sep='\\t', index=False)\n",
    "\n",
    "\n",
    "def get_repertoire_ids(data_dir: str) -> list:\n",
    "    \"\"\"\n",
    "    Retrieves repertoire IDs from the metadata file or filenames in the directory.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): The path to the data directory.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of repertoire IDs.\n",
    "    \"\"\"\n",
    "    metadata_path = os.path.join(data_dir, 'metadata.csv')\n",
    "\n",
    "    if os.path.exists(metadata_path):\n",
    "        metadata_df = pd.read_csv(metadata_path)\n",
    "        repertoire_ids = metadata_df['repertoire_id'].tolist()\n",
    "    else:\n",
    "        search_pattern = os.path.join(data_dir, '*.tsv')\n",
    "        tsv_files = glob.glob(search_pattern)\n",
    "        repertoire_ids = [os.path.basename(f).replace('.tsv', '') for f in sorted(tsv_files)]\n",
    "\n",
    "    return repertoire_ids\n",
    "\n",
    "\n",
    "def generate_random_top_sequences_df(n_seq: int = 50000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a random DataFrame simulating top important sequences.\n",
    "\n",
    "    Args:\n",
    "        n_seq (int): Number of sequences to generate.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with columns 'ID', 'dataset', 'junction_aa', 'v_call', 'j_call'.\n",
    "    \"\"\"\n",
    "    seqs = set()\n",
    "    while len(seqs) < n_seq:\n",
    "        seq = ''.join(np.random.choice(list('ACDEFGHIKLMNPQRSTVWY'), size=15))\n",
    "        seqs.add(seq)\n",
    "    data = {\n",
    "        'junction_aa': list(seqs),\n",
    "        'v_call': ['TRBV20-1'] * n_seq,\n",
    "        'j_call': ['TRBJ2-7'] * n_seq,\n",
    "        'importance_score': np.random.rand(n_seq)\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def validate_dirs_and_files(train_dir: str, test_dirs: List[str], out_dir: str) -> None:\n",
    "    assert os.path.isdir(train_dir), f\"Train directory `{train_dir}` does not exist.\"\n",
    "    train_tsvs = glob.glob(os.path.join(train_dir, \"*.tsv\"))\n",
    "    assert train_tsvs, f\"No .tsv files found in train directory `{train_dir}`.\"\n",
    "    metadata_path = os.path.join(train_dir, \"metadata.csv\")\n",
    "    assert os.path.isfile(metadata_path), f\"`metadata.csv` not found in train directory `{train_dir}`.\"\n",
    "\n",
    "    for test_dir in test_dirs:\n",
    "        assert os.path.isdir(test_dir), f\"Test directory `{test_dir}` does not exist.\"\n",
    "        test_tsvs = glob.glob(os.path.join(test_dir, \"*.tsv\"))\n",
    "        assert test_tsvs, f\"No .tsv files found in test directory `{test_dir}`.\"\n",
    "\n",
    "    try:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        test_file = os.path.join(out_dir, \"test_write_permission.tmp\")\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create or write to output directory `{out_dir}`: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def concatenate_output_files(out_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Concatenates all test predictions and important sequences TSV files from the output directory.\n",
    "\n",
    "    This function finds all files matching the patterns:\n",
    "    - *_test_predictions.tsv\n",
    "    - *_important_sequences.tsv\n",
    "\n",
    "    and concatenates them to match the expected output format of submissions.csv.\n",
    "\n",
    "    Args:\n",
    "        out_dir (str): Path to the output directory containing the TSV files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated DataFrame with predictions followed by important sequences.\n",
    "                     Columns: ['ID', 'dataset', 'label_positive_probability', 'junction_aa', 'v_call', 'j_call']\n",
    "    \"\"\"\n",
    "    predictions_pattern = os.path.join(out_dir, '*_test_predictions.tsv')\n",
    "    sequences_pattern = os.path.join(out_dir, '*_important_sequences.tsv')\n",
    "\n",
    "    predictions_files = sorted(glob.glob(predictions_pattern))\n",
    "    sequences_files = sorted(glob.glob(sequences_pattern))\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for pred_file in predictions_files:\n",
    "        try:\n",
    "            df = pd.read_csv(pred_file, sep='\\t')\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not read predictions file '{pred_file}'. Error: {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "    for seq_file in sequences_files:\n",
    "        try:\n",
    "            df = pd.read_csv(seq_file, sep='\\t')\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not read sequences file '{seq_file}'. Error: {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "    if not df_list:\n",
    "        print(\"Warning: No output files were found to concatenate.\")\n",
    "        concatenated_df = pd.DataFrame(\n",
    "            columns=['ID', 'dataset', 'label_positive_probability', 'junction_aa', 'v_call', 'j_call'])\n",
    "    else:\n",
    "        concatenated_df = pd.concat(df_list, ignore_index=True)\n",
    "    submissions_file = os.path.join(out_dir, 'submissions.csv')\n",
    "    concatenated_df.to_csv(submissions_file, index=False)\n",
    "    print(f\"Concatenated output written to `{submissions_file}`.\")\n",
    "\n",
    "\n",
    "def get_dataset_pairs(train_dir: str, test_dir: str) -> List[Tuple[str, List[str]]]:\n",
    "    \"\"\"Returns list of (train_path, [test_paths]) tuples for dataset pairs.\"\"\"\n",
    "    test_groups = defaultdict(list)\n",
    "    for test_name in sorted(os.listdir(test_dir)):\n",
    "        if test_name.startswith(\"test_dataset_\"):\n",
    "            base_id = test_name.replace(\"test_dataset_\", \"\").split(\"_\")[0]\n",
    "            test_groups[base_id].append(os.path.join(test_dir, test_name))\n",
    "\n",
    "    pairs = []\n",
    "    for train_name in sorted(os.listdir(train_dir)):\n",
    "        if train_name.startswith(\"train_dataset_\"):\n",
    "            train_id = train_name.replace(\"train_dataset_\", \"\")\n",
    "            train_path = os.path.join(train_dir, train_name)\n",
    "            pairs.append((train_path, test_groups.get(train_id, [])))\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Main ImmuneStatePredictor class, where participants will fill in their implementations within the placeholders \n",
    "## and replace any example code lines with actual code that makes sense\n",
    "\n",
    "\n",
    "class ImmuneStatePredictor:\n",
    "    \"\"\"\n",
    "    A template for predicting immune states from TCR repertoire data.\n",
    "\n",
    "    Participants should implement the logic for training, prediction, and\n",
    "    sequence identification within this class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_jobs: int = 1, device: str = 'cpu', **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the predictor.\n",
    "\n",
    "        Args:\n",
    "            n_jobs (int): Number of CPU cores to use for parallel processing.\n",
    "            device (str): The device to use for computation (e.g., 'cpu', 'cuda').\n",
    "            **kwargs: Additional hyperparameters for the model.\n",
    "        \"\"\"\n",
    "        total_cores = os.cpu_count()\n",
    "        if n_jobs == -1:\n",
    "            self.n_jobs = total_cores\n",
    "        else:\n",
    "            self.n_jobs = min(n_jobs, total_cores)\n",
    "        self.device = device\n",
    "        if device == 'cuda' and not torch.cuda.is_available():\n",
    "            print(\"Warning: 'cuda' was requested but is not available. Falling back to 'cpu'.\")\n",
    "            self.device = 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "        # --- your code starts here ---\n",
    "        # Example: Store hyperparameters, the actual model, identified important sequences, etc.\n",
    "\n",
    "        # NOTE: we encourage you to use self.n_jobs and self.device if appropriate in\n",
    "        # your implementation instead of hardcoding these values because your code may later be run in an\n",
    "        # environment with different hardware resources.\n",
    "\n",
    "        self.model = None\n",
    "        self.important_sequences_ = None\n",
    "        # --- your code ends here ---\n",
    "\n",
    "    def fit(self, train_dir_path: str):\n",
    "        \"\"\"\n",
    "        Trains the model on the provided training data.\n",
    "\n",
    "        Args:\n",
    "            train_dir_path (str): Path to the directory with training TSV files.\n",
    "\n",
    "        Returns:\n",
    "            self: The fitted predictor instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # --- your code starts here ---\n",
    "        # Load the data, prepare suited representations as needed, train your model,\n",
    "        # and find the top k important sequences that best explain the labels.\n",
    "        # Example: Load the data. One possibility could be to use the provided utility function as shown below.\n",
    "\n",
    "        # full_train_dataset_df = load_full_dataset(train_dir_path)\n",
    "\n",
    "        #   Model Training\n",
    "        #    Example: self.model = SomeClassifier().fit(X_train, y_train)\n",
    "        self.model = \"some trained model\"  # Replace with your actual learnt model\n",
    "\n",
    "        #   Identify important sequences (can be done here or in the dedicated method)\n",
    "        #    Example:\n",
    "        self.important_sequences_ = self.identify_associated_sequences(top_k=50000, dataset_name=os.path.basename(train_dir_path))\n",
    "\n",
    "        # --- your code ends here ---\n",
    "        print(\"Training complete.\")\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, test_dir_path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Predicts probabilities for examples in the provided path.\n",
    "\n",
    "        Args:\n",
    "            test_dir_path (str): Path to the directory with test TSV files.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame with 'ID', 'dataset', 'label_positive_probability', 'junction_aa', 'v_call', 'j_call' columns.\n",
    "        \"\"\"\n",
    "        print(f\"Making predictions for data in {test_dir_path}...\")\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"The model has not been fitted yet. Please call `fit` first.\")\n",
    "\n",
    "        # --- your code starts here ---\n",
    "\n",
    "        # Example: Load the data. One possibility could be to use the provided utility function as shown below.\n",
    "\n",
    "        # full_test_dataset_df = load_full_dataset(test_dir_path)\n",
    "        repertoire_ids = get_repertoire_ids(test_dir_path)  # Replace with actual repertoire IDs from the test data\n",
    "\n",
    "        # Prediction\n",
    "        #    Example:\n",
    "        # draw random probabilities for demonstration purposes\n",
    "\n",
    "        probabilities = np.random.rand(len(repertoire_ids)) # Replace with true predicted probabilities from your model\n",
    "\n",
    "        # --- your code ends here ---\n",
    "\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'ID': repertoire_ids,\n",
    "            'dataset': [os.path.basename(test_dir_path)] * len(repertoire_ids),\n",
    "            'label_positive_probability': probabilities\n",
    "        })\n",
    "\n",
    "        # to enable compatibility with the expected output format that includes junction_aa, v_call, j_call columns\n",
    "        predictions_df['junction_aa'] = -999.0\n",
    "        predictions_df['v_call'] = -999.0\n",
    "        predictions_df['j_call'] = -999.0\n",
    "\n",
    "        predictions_df = predictions_df[['ID', 'dataset', 'label_positive_probability', 'junction_aa', 'v_call', 'j_call']]\n",
    "\n",
    "        print(f\"Prediction complete on {len(repertoire_ids)} examples in {test_dir_path}.\")\n",
    "        return predictions_df\n",
    "\n",
    "    def identify_associated_sequences(self, dataset_name: str, top_k: int = 50000) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Identifies the top \"k\" important sequences (rows) from the training data that best explain the labels.\n",
    "\n",
    "        Args:\n",
    "            top_k (int): The number of top sequences to return (based on some scoring mechanism).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame with 'ID', 'dataset', 'label_positive_probability', 'junction_aa', 'v_call', 'j_call' columns.\n",
    "        \"\"\"\n",
    "\n",
    "        # --- your code starts here ---\n",
    "        \n",
    "        # Return the top k sequences, sorted based on some form of importance score.\n",
    "        # Example:\n",
    "        # all_sequences_scored = self._score_all_sequences()\n",
    "        \n",
    "        all_sequences_scored = generate_random_top_sequences_df(n_seq=top_k)  # Replace with your way of identifying top k sequences\n",
    "\n",
    "        # note that all_sequences_scored should contain a 'importance_score' column that will be used further below\n",
    "        \n",
    "        # --- your code ends here ---\n",
    "\n",
    "        top_sequences_df = all_sequences_scored.nlargest(top_k, 'importance_score')\n",
    "        top_sequences_df = top_sequences_df[['junction_aa', 'v_call', 'j_call']]\n",
    "        top_sequences_df['dataset'] = dataset_name\n",
    "        top_sequences_df['ID'] = range(1, len(top_sequences_df)+1)\n",
    "        top_sequences_df['ID'] = top_sequences_df['dataset'] + '_seq_top_' + top_sequences_df['ID'].astype(str)\n",
    "        top_sequences_df['label_positive_probability'] = -999.0 # to enable compatibility with the expected output format\n",
    "        top_sequences_df = top_sequences_df[['ID', 'dataset', 'label_positive_probability', 'junction_aa', 'v_call', 'j_call']]\n",
    "\n",
    "        return top_sequences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## The `main` workflow that uses your implementation of the ImmuneStatePredictor class to train, identify important sequences and predict test labels\n",
    "\n",
    "\n",
    "def _train_predictor(predictor: ImmuneStatePredictor, train_dir: str):\n",
    "    \"\"\"Trains the predictor on the training data.\"\"\"\n",
    "    print(f\"Fitting model on examples in ` {train_dir} `...\")\n",
    "    predictor.fit(train_dir)\n",
    "\n",
    "\n",
    "def _generate_predictions(predictor: ImmuneStatePredictor, test_dirs: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Generates predictions for all test directories and concatenates them.\"\"\"\n",
    "    all_preds = []\n",
    "    for test_dir in test_dirs:\n",
    "        print(f\"Predicting on examples in ` {test_dir} `...\")\n",
    "        preds = predictor.predict_proba(test_dir)\n",
    "        if preds is not None and not preds.empty:\n",
    "            all_preds.append(preds)\n",
    "        else:\n",
    "            print(f\"Warning: No predictions returned for {test_dir}\")\n",
    "    if all_preds:\n",
    "        return pd.concat(all_preds, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def _save_predictions(predictions: pd.DataFrame, out_dir: str, train_dir: str) -> None:\n",
    "    \"\"\"Saves predictions to a TSV file.\"\"\"\n",
    "    if predictions.empty:\n",
    "        raise ValueError(\"No predictions to save - predictions DataFrame is empty\")\n",
    "\n",
    "    preds_path = os.path.join(out_dir, f\"{os.path.basename(train_dir)}_test_predictions.tsv\")\n",
    "    save_tsv(predictions, preds_path)\n",
    "    print(f\"Predictions written to `{preds_path}`.\")\n",
    "\n",
    "\n",
    "def _save_important_sequences(predictor: ImmuneStatePredictor, out_dir: str, train_dir: str) -> None:\n",
    "    \"\"\"Saves important sequences to a TSV file.\"\"\"\n",
    "    seqs = predictor.important_sequences_\n",
    "    if seqs is None or seqs.empty:\n",
    "        raise ValueError(\"No important sequences available to save\")\n",
    "\n",
    "    seqs_path = os.path.join(out_dir, f\"{os.path.basename(train_dir)}_important_sequences.tsv\")\n",
    "    save_tsv(seqs, seqs_path)\n",
    "    print(f\"Important sequences written to `{seqs_path}`.\")\n",
    "\n",
    "\n",
    "def main(train_dir: str, test_dirs: List[str], out_dir: str, n_jobs: int, device: str) -> None:\n",
    "    validate_dirs_and_files(train_dir, test_dirs, out_dir)\n",
    "    predictor = ImmuneStatePredictor(n_jobs=n_jobs,\n",
    "                                     device=device)  # instantiate with any other parameters as defined by you in the class\n",
    "    _train_predictor(predictor, train_dir)\n",
    "    predictions = _generate_predictions(predictor, test_dirs)\n",
    "    _save_predictions(predictions, out_dir, train_dir)\n",
    "    _save_important_sequences(predictor, out_dir, train_dir)\n",
    "\n",
    "\n",
    "def run():\n",
    "    parser = argparse.ArgumentParser(description=\"Immune State Predictor CLI\")\n",
    "    parser.add_argument(\"--train_dir\", required=True, help=\"Path to training data directory\")\n",
    "    parser.add_argument(\"--test_dirs\", required=True, nargs=\"+\", help=\"Path(s) to test data director(ies)\")\n",
    "    parser.add_argument(\"--out_dir\", required=True, help=\"Path to output directory\")\n",
    "    parser.add_argument(\"--n_jobs\", type=int, default=1,\n",
    "                        help=\"Number of CPU cores to use. Use -1 for all available cores.\")\n",
    "    parser.add_argument(\"--device\", type=str, default='cpu', choices=['cpu', 'cuda'],\n",
    "                        help=\"Device to use for computation ('cpu' or 'cuda').\")\n",
    "    args = parser.parse_args()\n",
    "    main(args.train_dir, args.test_dirs, args.out_dir, args.n_jobs, args.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_datasets_dir = \"/kaggle/input/adaptive-immune-profiling-challenge-2025/train_datasets/train_datasets\"\n",
    "test_datasets_dir = \"/kaggle/input/adaptive-immune-profiling-challenge-2025/test_datasets/test_datasets\"\n",
    "results_dir = \"/kaggle/working/results\"\n",
    "\n",
    "train_test_dataset_pairs = get_dataset_pairs(train_datasets_dir, test_datasets_dir)\n",
    "\n",
    "for train_dir, test_dirs in train_test_dataset_pairs:\n",
    "    main(train_dir=train_dir, test_dirs=test_dirs, out_dir=results_dir, n_jobs=4, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "concatenate_output_files(out_dir=results_dir)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13374319,
     "sourceId": 106680,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
